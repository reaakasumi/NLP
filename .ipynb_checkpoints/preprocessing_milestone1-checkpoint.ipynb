{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563bb29e",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1e3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from scipy.stats import spearmanr\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import stanza\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stanza.utils.conll import CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d910ba86-fa7d-4092-954e-8688b434a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008511781692504883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f634110c50f94af1996977557e6d167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:42:45 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-03 15:42:45 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-11-03 15:42:46 INFO: File exists: C:\\Users\\Admin\\stanza_resources\\en\\default.zip\n",
      "2024-11-03 15:42:49 INFO: Finished downloading models and saved to C:\\Users\\Admin\\stanza_resources\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060024261474609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86b787513854d35a69e45ae9c4f1790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:42:50 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-03 15:42:50 INFO: Downloading default packages for language: de (German) ...\n",
      "2024-11-03 15:42:51 INFO: File exists: C:\\Users\\Admin\\stanza_resources\\de\\default.zip\n",
      "2024-11-03 15:42:55 INFO: Finished downloading models and saved to C:\\Users\\Admin\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK and Stanza models\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('en')\n",
    "stanza.download('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc985ab",
   "metadata": {},
   "source": [
    "### Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9e39a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hyp  \\\n",
       "0   Don't worry, it's only temporary.   \n",
       "1    Tom is never where he should be.   \n",
       "2  It's hard for me to work with Tom.   \n",
       "3                      Water, please.   \n",
       "4   I didn't expect Tom to betray me.   \n",
       "\n",
       "                                        tgt  \\\n",
       "0         Don't worry. It's only temporary.   \n",
       "1   Tom is never where he's supposed to be.   \n",
       "2          I have trouble working with Tom.   \n",
       "3                      I'd like some water.   \n",
       "4  I didn't think that Tom would betray me.   \n",
       "\n",
       "                                         src     ref task model  \n",
       "0          Не волнуйся. Это только временно.  either   MT        \n",
       "1  Тома никогда нет там, где он должен быть.  either   MT        \n",
       "2               Мне сложно работать с Томом.  either   MT        \n",
       "3                          Воду, пожалуйста.  either   MT        \n",
       "4         Я не ожидал, что Том предаст меня.  either   MT        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_json('train.model-agnostic.json')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afc8f6d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Of or pertaining to the language of a particul...</td>\n",
       "      <td>Of or pertaining to everyday language , as opp...</td>\n",
       "      <td>There are blacktips , silvertips , bronze whal...</td>\n",
       "      <td>tgt</td>\n",
       "      <td>DM</td>\n",
       "      <td>ltg/flan-t5-definition-en-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not coercive ; not involving coercion</td>\n",
       "      <td>Not coercive ; free of coercion</td>\n",
       "      <td>Mr. obama signed executive orders requiring al...</td>\n",
       "      <td>tgt</td>\n",
       "      <td>DM</td>\n",
       "      <td>ltg/flan-t5-definition-en-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>To express or express by words ; to express by...</td>\n",
       "      <td>To depict or portray .</td>\n",
       "      <td>Disloyal ? / the word is too good to paint out...</td>\n",
       "      <td>tgt</td>\n",
       "      <td>DM</td>\n",
       "      <td>ltg/flan-t5-definition-en-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Having the power to authoritatively speak or w...</td>\n",
       "      <td>Having a commanding style .</td>\n",
       "      <td>He instructed us in that booming , authoritati...</td>\n",
       "      <td>tgt</td>\n",
       "      <td>DM</td>\n",
       "      <td>ltg/flan-t5-definition-en-base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Without a scot .</td>\n",
       "      <td>Without consequences or penalties , to go free...</td>\n",
       "      <td>To get off scot-free . What is the meaning of ...</td>\n",
       "      <td>tgt</td>\n",
       "      <td>DM</td>\n",
       "      <td>ltg/flan-t5-definition-en-base</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 hyp  \\\n",
       "0  Of or pertaining to the language of a particul...   \n",
       "1              Not coercive ; not involving coercion   \n",
       "2  To express or express by words ; to express by...   \n",
       "3  Having the power to authoritatively speak or w...   \n",
       "4                                   Without a scot .   \n",
       "\n",
       "                                                 tgt  \\\n",
       "0  Of or pertaining to everyday language , as opp...   \n",
       "1                    Not coercive ; free of coercion   \n",
       "2                             To depict or portray .   \n",
       "3                        Having a commanding style .   \n",
       "4  Without consequences or penalties , to go free...   \n",
       "\n",
       "                                                 src  ref task  \\\n",
       "0  There are blacktips , silvertips , bronze whal...  tgt   DM   \n",
       "1  Mr. obama signed executive orders requiring al...  tgt   DM   \n",
       "2  Disloyal ? / the word is too good to paint out...  tgt   DM   \n",
       "3  He instructed us in that booming , authoritati...  tgt   DM   \n",
       "4  To get off scot-free . What is the meaning of ...  tgt   DM   \n",
       "\n",
       "                            model  \n",
       "0  ltg/flan-t5-definition-en-base  \n",
       "1  ltg/flan-t5-definition-en-base  \n",
       "2  ltg/flan-t5-definition-en-base  \n",
       "3  ltg/flan-t5-definition-en-base  \n",
       "4  ltg/flan-t5-definition-en-base  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_dfx = pd.read_json('train.model-aware.v2.json')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "train_dfx.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e9156",
   "metadata": {},
   "source": [
    "### Step 3: Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a764aaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp      0\n",
      "tgt      0\n",
      "src      0\n",
      "ref      0\n",
      "task     0\n",
      "model    0\n",
      "dtype: int64\n",
      "Unique values in 'task' column: ['MT' 'DM' 'PG']\n",
      "Unique values in 'ref' column: ['either' 'tgt' 'src']\n",
      "Unique values in 'model' column: ['']\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Get unique values in categorical columns\n",
    "print(\"Unique values in 'task' column:\", train_df['task'].unique())\n",
    "print(\"Unique values in 'ref' column:\", train_df['ref'].unique())\n",
    "print(\"Unique values in 'model' column:\", train_df['model'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515287d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Distribution:\n",
      " MT    10000\n",
      "PG    10000\n",
      "DM    10000\n",
      "Name: task, dtype: int64\n",
      "Hypothesis Text Length Stats:\n",
      " count    30000.000000\n",
      "mean         5.570867\n",
      "std          3.401798\n",
      "min          1.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%          7.000000\n",
      "max        108.000000\n",
      "Name: hyp_length, dtype: float64\n",
      "Target Text Length Stats:\n",
      " count    30000.000000\n",
      "mean         5.325933\n",
      "std          6.035543\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          5.000000\n",
      "75%          8.000000\n",
      "max         89.000000\n",
      "Name: tgt_length, dtype: float64\n",
      "Source Text Length Stats:\n",
      " count    30000.000000\n",
      "mean        14.673300\n",
      "std         19.259339\n",
      "min          1.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%         21.000000\n",
      "max        457.000000\n",
      "Name: src_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check data distribution in the 'task' column\n",
    "print(\"Task Distribution:\\n\", train_df['task'].value_counts())\n",
    "\n",
    "# Look at text length distributions for hyp, tgt, and src columns\n",
    "train_df['hyp_length'] = train_df['hyp'].apply(lambda x: len(x.split()))\n",
    "train_df['tgt_length'] = train_df['tgt'].apply(lambda x: len(x.split()))\n",
    "train_df['src_length'] = train_df['src'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Summary statistics of lengths\n",
    "print(\"Hypothesis Text Length Stats:\\n\", train_df['hyp_length'].describe())\n",
    "print(\"Target Text Length Stats:\\n\", train_df['tgt_length'].describe())\n",
    "print(\"Source Text Length Stats:\\n\", train_df['src_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "384009b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reference types used by each model:\n",
      "  task       ref\n",
      "0   DM     [tgt]\n",
      "1   MT  [either]\n",
      "2   PG     [src]\n"
     ]
    }
   ],
   "source": [
    "# Group by model and list unique reference types for each model\n",
    "model_reference_usage = train_df.groupby('task')['ref'].unique().reset_index()\n",
    "\n",
    "# Display reference types used by each model\n",
    "print(\"Reference types used by each task:\")\n",
    "print(model_reference_usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00972804",
   "metadata": {},
   "source": [
    "Based on the results, we will apply preprocessing steps accordingly to the specific task requirements: for **Definition Modeling (DM)** tasks, we will focus on the target (`tgt`) as the main reference, for **Paraphrase Generation (PG)** tasks, we will use the source (`src`) as the reference, and for **Machine Translation (MT)** tasks, we can consider either the source or the target as references, but for our analysis we will continue with (`src`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d016f76",
   "metadata": {},
   "source": [
    "### Step 4: Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14da51",
   "metadata": {},
   "source": [
    "#### Tokenization and Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97de8806-2b39-40cb-9b55-64202e7b3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 23923), ('a', 7651), ('(', 7035), (')', 7034), (',', 6650), ('to', 6608), ('of', 6476), ('the', 5871), ('you', 4853), ('i', 4679), ('?', 4486), (\"'s\", 3047), ('tom', 2370), ('or', 2217), ('in', 2161), (\"n't\", 2109), ('do', 1924), ('it', 1844), ('is', 1586), ('that', 1489)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text and count word frequencies\n",
    "words = [word.lower() for text in train_df['hyp'] for word in word_tokenize(text)]\n",
    "word_counts = Counter(words)\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce2475-2e00-435f-922a-0627ea635139",
   "metadata": {},
   "source": [
    "Word frequency analysis reveals that punctuation and common function words dominate the top results. These typically don't contribute much meaningful information for analysis in tasks like hallucination detection. So \n",
    "Remove Punctuatio and \n",
    "Remove Stopworh.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54af087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Text Normalization\n",
    "def tokenize_and_normalize(text):\n",
    "    tokens = word_tokenize(text.lower())  # Lowercasing\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # Removing punctuation (fshije)\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]  # Removing stopwords (fshije)\n",
    "    return tokens\n",
    "\n",
    "train_df['hyp_tokens'] = train_df['hyp'].apply(tokenize_and_normalize)\n",
    "train_df['tgt_tokens'] = train_df['tgt'].apply(tokenize_and_normalize)\n",
    "train_df['src_tokens'] = train_df['src'].apply(tokenize_and_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a41b66d-46c0-4c93-be19-00eb743f35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tom', 2370), ('transitive', 899), ('pertaining', 834), ('informal', 738), ('one', 670), ('form', 649), ('know', 601), ('alternative', 587), ('obsolete', 570), ('something', 550)]\n"
     ]
    }
   ],
   "source": [
    "# Token frequency analysis\n",
    "hyp_word_counts = Counter([word for tokens in train_df['hyp_tokens'] for word in tokens])\n",
    "print(hyp_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730f70af-e821-4161-a1b0-efbb02d1acd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tom', 2506), ('one', 802), ('rare', 510), ('slang', 445), ('obsolete', 421), ('like', 416), ('transitive', 412), ('something', 403), ('form', 390), ('mary', 371)]\n"
     ]
    }
   ],
   "source": [
    "# Token frequency analysis\n",
    "tgr_word_counts = Counter([word for tokens in train_df['tgt_tokens'] for word in tokens])\n",
    "print(tgr_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08dad5ff-42f0-4e07-87b9-b542ab1f21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('define', 10008), ('не', 2228), ('я', 2027), ('том', 1682), ('что', 1182), ('в', 1139), ('one', 973), ('это', 923), ('ты', 817), ('на', 758)]\n"
     ]
    }
   ],
   "source": [
    "# Token frequency analysis\n",
    "src_word_counts = Counter([word for tokens in train_df['src_tokens'] for word in tokens])\n",
    "print(src_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e7d1ec-b3ed-467f-abae-476daca134d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>hyp_length</th>\n",
       "      <th>tgt_length</th>\n",
       "      <th>src_length</th>\n",
       "      <th>hyp_tokens</th>\n",
       "      <th>tgt_tokens</th>\n",
       "      <th>src_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[не, волнуйся, это, только, временно]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[tom, never]</td>\n",
       "      <td>[tom, never, supposed]</td>\n",
       "      <td>[тома, никогда, нет, там, где, он, должен, быть]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>[hard, work, tom]</td>\n",
       "      <td>[trouble, working, tom]</td>\n",
       "      <td>[мне, сложно, работать, с, томом]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[water, please]</td>\n",
       "      <td>[like, water]</td>\n",
       "      <td>[воду, пожалуйста]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>[expect, tom, betray]</td>\n",
       "      <td>[think, tom, would, betray]</td>\n",
       "      <td>[я, не, ожидал, что, том, предаст, меня]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hyp  \\\n",
       "0   Don't worry, it's only temporary.   \n",
       "1    Tom is never where he should be.   \n",
       "2  It's hard for me to work with Tom.   \n",
       "3                      Water, please.   \n",
       "4   I didn't expect Tom to betray me.   \n",
       "\n",
       "                                        tgt  \\\n",
       "0         Don't worry. It's only temporary.   \n",
       "1   Tom is never where he's supposed to be.   \n",
       "2          I have trouble working with Tom.   \n",
       "3                      I'd like some water.   \n",
       "4  I didn't think that Tom would betray me.   \n",
       "\n",
       "                                         src     ref task model  hyp_length  \\\n",
       "0          Не волнуйся. Это только временно.  either   MT                 5   \n",
       "1  Тома никогда нет там, где он должен быть.  either   MT                 7   \n",
       "2               Мне сложно работать с Томом.  either   MT                 8   \n",
       "3                          Воду, пожалуйста.  either   MT                 2   \n",
       "4         Я не ожидал, что Том предаст меня.  either   MT                 7   \n",
       "\n",
       "   tgt_length  src_length             hyp_tokens                   tgt_tokens  \\\n",
       "0           5           5     [worry, temporary]           [worry, temporary]   \n",
       "1           8           8           [tom, never]       [tom, never, supposed]   \n",
       "2           6           5      [hard, work, tom]      [trouble, working, tom]   \n",
       "3           4           2        [water, please]                [like, water]   \n",
       "4           8           7  [expect, tom, betray]  [think, tom, would, betray]   \n",
       "\n",
       "                                         src_tokens  \n",
       "0             [не, волнуйся, это, только, временно]  \n",
       "1  [тома, никогда, нет, там, где, он, должен, быть]  \n",
       "2                 [мне, сложно, работать, с, томом]  \n",
       "3                                [воду, пожалуйста]  \n",
       "4          [я, не, ожидал, что, том, предаст, меня]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a60fc0",
   "metadata": {},
   "source": [
    "#### Search for Specific Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b42c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with the number seven: ['He was born at 7 a.m. on June 5, 1970.', 'The population of Hong Kong is more than seven million people.', 'In 1951 at the Palace of Soviet Pioneers, British international master Robert Wade held a session of simultaneous play with 30 local children under 14 years old. After seven hours of MI Wade game, he managed to make 10 draws, losing the remaining 20 matches.', 'My mother had seven sons and four daughters, and she had five sisters.', 'My mother had seven sons and four daughters, and she had five sisters.']\n"
     ]
    }
   ],
   "source": [
    "# Search for Specific Patterns\n",
    "def search_text_column(pattern, data, column='hyp'):\n",
    "    return [row[column] for idx, row in data.iterrows() if re.search(pattern, row[column])]\n",
    "\n",
    "# Example: Find rows in 'hyp' containing numbers\n",
    "hyp_with_numbers = search_text_column(r'\\d+', train_df, column='hyp')\n",
    "\n",
    "# Pattern matching using regular expressions\n",
    "def search_pattern(pattern, column, data=train_df):\n",
    "    matches = []\n",
    "    for _, row in data.iterrows():\n",
    "        match = re.search(pattern, row[column])\n",
    "        if match:\n",
    "            matches.append(row[column])\n",
    "    return matches\n",
    "\n",
    "# Example pattern search\n",
    "matches = search_pattern(r'\\b(seven|7)\\b', 'hyp')\n",
    "print(\"Examples with the number seven:\", matches[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b76119",
   "metadata": {},
   "source": [
    "#### Length Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27888274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length Analysis (outlier detection)\n",
    "train_df['hyp_length'] = train_df['hyp'].apply(lambda x: len(x.split()))\n",
    "train_df['src_length'] = train_df['src'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16560f05-dbed-4161-b639-22d3b2c864bc",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10f34f5-d898-406d-8602-e878740332f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:45:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007513523101806641,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1d0f18f9b1468693caf720912aab52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:45:38 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-03 15:45:38 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-03 15:45:38 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-03 15:45:38 INFO: Using device: cpu\n",
      "2024-11-03 15:45:38 INFO: Loading: tokenize\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 15:45:38 INFO: Loading: mwt\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 15:45:38 INFO: Loading: lemma\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 15:45:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using Stanza\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92146ed4-05da-438e-a68c-ca5e837e2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['hyp_lemmas'] = train_df['hyp'].apply(lambda x: ' '.join(lemmatize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541fa1c6-97e3-494d-956a-7f98cbad7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tgt_lemmas'] = train_df['tgt'].apply(lambda x: ' '.join(lemmatize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc7ed140-b9ec-4cfc-95c8-4cb3b7a2bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the 'src' column only if the task is not 'ML'\n",
    "train_df['src_lemmas'] = train_df.apply(lambda row: ' '.join(lemmatize_text(row['src'])) if row['task'] != 'ML' else row['src'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a629655-92f6-4f45-b4e8-368705a938e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>hyp_length</th>\n",
       "      <th>tgt_length</th>\n",
       "      <th>src_length</th>\n",
       "      <th>hyp_tokens</th>\n",
       "      <th>tgt_tokens</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>hyp_lemmas</th>\n",
       "      <th>tgt_lemmas</th>\n",
       "      <th>src_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[не, волнуйся, это, только, временно]</td>\n",
       "      <td>do not worry , it 's only temporary .</td>\n",
       "      <td>do not worry . it 's only temporary .</td>\n",
       "      <td>Нi deлнуeсy . Это eeлькe deемеeнy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[tom, never]</td>\n",
       "      <td>[tom, never, supposed]</td>\n",
       "      <td>[тома, никогда, нет, там, где, он, должен, быть]</td>\n",
       "      <td>Tom be never where he should be .</td>\n",
       "      <td>Tom be never where he 's suppose to be .</td>\n",
       "      <td>Тома eик нет там , где оi eeлжеe быть .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>[hard, work, tom]</td>\n",
       "      <td>[trouble, working, tom]</td>\n",
       "      <td>[мне, сложно, работать, с, томом]</td>\n",
       "      <td>it 's hard for I to work with Tom .</td>\n",
       "      <td>I have trouble work with Tom .</td>\n",
       "      <td>Мне eeожнe deботeтy с Томом .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[water, please]</td>\n",
       "      <td>[like, water]</td>\n",
       "      <td>[воду, пожалуйста]</td>\n",
       "      <td>water , please .</td>\n",
       "      <td>I would like some water .</td>\n",
       "      <td>Воду , dожаeуйсeа .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>[expect, tom, betray]</td>\n",
       "      <td>[think, tom, would, betray]</td>\n",
       "      <td>[я, не, ожидал, что, том, предаст, меня]</td>\n",
       "      <td>I do not expect Tom to betray I .</td>\n",
       "      <td>I do not think that Tom would betray I .</td>\n",
       "      <td>Я нi eeидаe , что Том eре меня .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hyp  \\\n",
       "0   Don't worry, it's only temporary.   \n",
       "1    Tom is never where he should be.   \n",
       "2  It's hard for me to work with Tom.   \n",
       "3                      Water, please.   \n",
       "4   I didn't expect Tom to betray me.   \n",
       "\n",
       "                                        tgt  \\\n",
       "0         Don't worry. It's only temporary.   \n",
       "1   Tom is never where he's supposed to be.   \n",
       "2          I have trouble working with Tom.   \n",
       "3                      I'd like some water.   \n",
       "4  I didn't think that Tom would betray me.   \n",
       "\n",
       "                                         src     ref task model  hyp_length  \\\n",
       "0          Не волнуйся. Это только временно.  either   MT                 5   \n",
       "1  Тома никогда нет там, где он должен быть.  either   MT                 7   \n",
       "2               Мне сложно работать с Томом.  either   MT                 8   \n",
       "3                          Воду, пожалуйста.  either   MT                 2   \n",
       "4         Я не ожидал, что Том предаст меня.  either   MT                 7   \n",
       "\n",
       "   tgt_length  src_length             hyp_tokens                   tgt_tokens  \\\n",
       "0           5           5     [worry, temporary]           [worry, temporary]   \n",
       "1           8           8           [tom, never]       [tom, never, supposed]   \n",
       "2           6           5      [hard, work, tom]      [trouble, working, tom]   \n",
       "3           4           2        [water, please]                [like, water]   \n",
       "4           8           7  [expect, tom, betray]  [think, tom, would, betray]   \n",
       "\n",
       "                                         src_tokens  \\\n",
       "0             [не, волнуйся, это, только, временно]   \n",
       "1  [тома, никогда, нет, там, где, он, должен, быть]   \n",
       "2                 [мне, сложно, работать, с, томом]   \n",
       "3                                [воду, пожалуйста]   \n",
       "4          [я, не, ожидал, что, том, предаст, меня]   \n",
       "\n",
       "                              hyp_lemmas  \\\n",
       "0  do not worry , it 's only temporary .   \n",
       "1      Tom be never where he should be .   \n",
       "2    it 's hard for I to work with Tom .   \n",
       "3                       water , please .   \n",
       "4      I do not expect Tom to betray I .   \n",
       "\n",
       "                                 tgt_lemmas  \\\n",
       "0     do not worry . it 's only temporary .   \n",
       "1  Tom be never where he 's suppose to be .   \n",
       "2            I have trouble work with Tom .   \n",
       "3                 I would like some water .   \n",
       "4  I do not think that Tom would betray I .   \n",
       "\n",
       "                                src_lemmas  \n",
       "0      Нi deлнуeсy . Это eeлькe deемеeнy .  \n",
       "1  Тома eик нет там , где оi eeлжеe быть .  \n",
       "2            Мне eeожнe deботeтy с Томом .  \n",
       "3                      Воду , dожаeуйсeа .  \n",
       "4         Я нi eeидаe , что Том eре меня .  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb895652-dc07-401a-8953-e75891cdf251",
   "metadata": {},
   "source": [
    "Optional Columns:\n",
    "- hyp_length, src_length \n",
    "- tgt_tokens, tgt_lemmas"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testenv)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
