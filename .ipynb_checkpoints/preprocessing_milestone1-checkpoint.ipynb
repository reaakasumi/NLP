{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563bb29e",
   "metadata": {},
   "source": [
    "### Step 1: Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b1e3842",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, log_loss\n",
    "from scipy.stats import spearmanr\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "from scipy.stats import spearmanr\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import stanza\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from stanza.utils.conll import CoNLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d910ba86-fa7d-4092-954e-8688b434a1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008511781692504883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f634110c50f94af1996977557e6d167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:42:45 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-03 15:42:45 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-11-03 15:42:46 INFO: File exists: C:\\Users\\Admin\\stanza_resources\\en\\default.zip\n",
      "2024-11-03 15:42:49 INFO: Finished downloading models and saved to C:\\Users\\Admin\\stanza_resources\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0060024261474609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86b787513854d35a69e45ae9c4f1790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:42:50 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-03 15:42:50 INFO: Downloading default packages for language: de (German) ...\n",
      "2024-11-03 15:42:51 INFO: File exists: C:\\Users\\Admin\\stanza_resources\\de\\default.zip\n",
      "2024-11-03 15:42:55 INFO: Finished downloading models and saved to C:\\Users\\Admin\\stanza_resources\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK and Stanza models\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stanza.download('en')\n",
    "stanza.download('de')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc985ab",
   "metadata": {},
   "source": [
    "### Step 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de9e39a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hyp  \\\n",
       "0   Don't worry, it's only temporary.   \n",
       "1    Tom is never where he should be.   \n",
       "2  It's hard for me to work with Tom.   \n",
       "3                      Water, please.   \n",
       "4   I didn't expect Tom to betray me.   \n",
       "\n",
       "                                        tgt  \\\n",
       "0         Don't worry. It's only temporary.   \n",
       "1   Tom is never where he's supposed to be.   \n",
       "2          I have trouble working with Tom.   \n",
       "3                      I'd like some water.   \n",
       "4  I didn't think that Tom would betray me.   \n",
       "\n",
       "                                         src     ref task model  \n",
       "0          Не волнуйся. Это только временно.  either   MT        \n",
       "1  Тома никогда нет там, где он должен быть.  either   MT        \n",
       "2               Мне сложно работать с Томом.  either   MT        \n",
       "3                          Воду, пожалуйста.  either   MT        \n",
       "4         Я не ожидал, что Том предаст меня.  either   MT        "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "train_df = pd.read_json('train.model-agnostic.json')\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842e9156",
   "metadata": {},
   "source": [
    "### Step 3: Exploratory Data Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a764aaad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyp      0\n",
      "tgt      0\n",
      "src      0\n",
      "ref      0\n",
      "task     0\n",
      "model    0\n",
      "dtype: int64\n",
      "Unique values in 'task' column: ['MT' 'DM' 'PG']\n",
      "Unique values in 'ref' column: ['either' 'tgt' 'src']\n",
      "Unique values in 'model' column: ['']\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(train_df.isnull().sum())\n",
    "\n",
    "# Get unique values in categorical columns\n",
    "print(\"Unique values in 'task' column:\", train_df['task'].unique())\n",
    "print(\"Unique values in 'ref' column:\", train_df['ref'].unique())\n",
    "print(\"Unique values in 'model' column:\", train_df['model'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515287d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Distribution:\n",
      " MT    10000\n",
      "PG    10000\n",
      "DM    10000\n",
      "Name: task, dtype: int64\n",
      "Hypothesis Text Length Stats:\n",
      " count    30000.000000\n",
      "mean         5.570867\n",
      "std          3.401798\n",
      "min          1.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%          7.000000\n",
      "max        108.000000\n",
      "Name: hyp_length, dtype: float64\n",
      "Target Text Length Stats:\n",
      " count    30000.000000\n",
      "mean         5.325933\n",
      "std          6.035543\n",
      "min          0.000000\n",
      "25%          0.000000\n",
      "50%          5.000000\n",
      "75%          8.000000\n",
      "max         89.000000\n",
      "Name: tgt_length, dtype: float64\n",
      "Source Text Length Stats:\n",
      " count    30000.000000\n",
      "mean        14.673300\n",
      "std         19.259339\n",
      "min          1.000000\n",
      "25%          4.000000\n",
      "50%          5.000000\n",
      "75%         21.000000\n",
      "max        457.000000\n",
      "Name: src_length, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Check data distribution in the 'task' column\n",
    "print(\"Task Distribution:\\n\", train_df['task'].value_counts())\n",
    "\n",
    "# Look at text length distributions for hyp, tgt, and src columns\n",
    "train_df['hyp_length'] = train_df['hyp'].apply(lambda x: len(x.split()))\n",
    "train_df['tgt_length'] = train_df['tgt'].apply(lambda x: len(x.split()))\n",
    "train_df['src_length'] = train_df['src'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Summary statistics of lengths\n",
    "print(\"Hypothesis Text Length Stats:\\n\", train_df['hyp_length'].describe())\n",
    "print(\"Target Text Length Stats:\\n\", train_df['tgt_length'].describe())\n",
    "print(\"Source Text Length Stats:\\n\", train_df['src_length'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d016f76",
   "metadata": {},
   "source": [
    "### Step 4: Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e14da51",
   "metadata": {},
   "source": [
    "#### Tokenization and Text Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97de8806-2b39-40cb-9b55-64202e7b3a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('.', 23923), ('a', 7651), ('(', 7035), (')', 7034), (',', 6650), ('to', 6608), ('of', 6476), ('the', 5871), ('you', 4853), ('i', 4679), ('?', 4486), (\"'s\", 3047), ('tom', 2370), ('or', 2217), ('in', 2161), (\"n't\", 2109), ('do', 1924), ('it', 1844), ('is', 1586), ('that', 1489)]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the text and count word frequencies\n",
    "words = [word.lower() for text in train_df['hyp'] for word in word_tokenize(text)]\n",
    "word_counts = Counter(words)\n",
    "print(word_counts.most_common(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ce2475-2e00-435f-922a-0627ea635139",
   "metadata": {},
   "source": [
    "Word frequency analysis reveals that punctuation and common function words dominate the top results. These typically don't contribute much meaningful information for analysis in tasks like hallucination detection. So \n",
    "Remove Punctuatio and \r\n",
    "Remove Stopworh.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54af087e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization and Text Normalization\n",
    "def tokenize_and_normalize(text):\n",
    "    tokens = word_tokenize(text.lower())  # Lowercasing\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # Removing punctuation (fshije)\n",
    "    tokens = [t for t in tokens if t not in stopwords.words('english')]  # Removing stopwords (fshije)\n",
    "    return tokens\n",
    "\n",
    "train_df['hyp_tokens'] = train_df['hyp'].apply(tokenize_and_normalize)\n",
    "train_df['tgt_tokens'] = train_df['tgt'].apply(tokenize_and_normalize)\n",
    "train_df['src_tokens'] = train_df['src'].apply(tokenize_and_normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a41b66d-46c0-4c93-be19-00eb743f35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tom', 2370), ('transitive', 899), ('pertaining', 834), ('informal', 738), ('one', 670), ('form', 649), ('know', 601), ('alternative', 587), ('obsolete', 570), ('something', 550)]\n"
     ]
    }
   ],
   "source": [
    "# Token frequency analysis\n",
    "hyp_word_counts = Counter([word for tokens in train_df['hyp_tokens'] for word in tokens])\n",
    "print(hyp_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "730f70af-e821-4161-a1b0-efbb02d1acd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tom', 2506), ('one', 802), ('rare', 510), ('slang', 445), ('obsolete', 421), ('like', 416), ('transitive', 412), ('something', 403), ('form', 390), ('mary', 371)]\n"
     ]
    }
   ],
   "source": [
    "# Token frequency analysis\n",
    "tgr_word_counts = Counter([word for tokens in train_df['tgt_tokens'] for word in tokens])\n",
    "print(tgr_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08dad5ff-42f0-4e07-87b9-b542ab1f21d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('define', 10008), ('не', 2228), ('я', 2027), ('том', 1682), ('что', 1182), ('в', 1139), ('one', 973), ('это', 923), ('ты', 817), ('на', 758)]\n"
     ]
    }
   ],
   "source": [
    "# Token frequency analysis\n",
    "src_word_counts = Counter([word for tokens in train_df['src_tokens'] for word in tokens])\n",
    "print(src_word_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e7d1ec-b3ed-467f-abae-476daca134d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>hyp_length</th>\n",
       "      <th>tgt_length</th>\n",
       "      <th>src_length</th>\n",
       "      <th>hyp_tokens</th>\n",
       "      <th>tgt_tokens</th>\n",
       "      <th>src_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[не, волнуйся, это, только, временно]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[tom, never]</td>\n",
       "      <td>[tom, never, supposed]</td>\n",
       "      <td>[тома, никогда, нет, там, где, он, должен, быть]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>[hard, work, tom]</td>\n",
       "      <td>[trouble, working, tom]</td>\n",
       "      <td>[мне, сложно, работать, с, томом]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[water, please]</td>\n",
       "      <td>[like, water]</td>\n",
       "      <td>[воду, пожалуйста]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>[expect, tom, betray]</td>\n",
       "      <td>[think, tom, would, betray]</td>\n",
       "      <td>[я, не, ожидал, что, том, предаст, меня]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hyp  \\\n",
       "0   Don't worry, it's only temporary.   \n",
       "1    Tom is never where he should be.   \n",
       "2  It's hard for me to work with Tom.   \n",
       "3                      Water, please.   \n",
       "4   I didn't expect Tom to betray me.   \n",
       "\n",
       "                                        tgt  \\\n",
       "0         Don't worry. It's only temporary.   \n",
       "1   Tom is never where he's supposed to be.   \n",
       "2          I have trouble working with Tom.   \n",
       "3                      I'd like some water.   \n",
       "4  I didn't think that Tom would betray me.   \n",
       "\n",
       "                                         src     ref task model  hyp_length  \\\n",
       "0          Не волнуйся. Это только временно.  either   MT                 5   \n",
       "1  Тома никогда нет там, где он должен быть.  either   MT                 7   \n",
       "2               Мне сложно работать с Томом.  either   MT                 8   \n",
       "3                          Воду, пожалуйста.  either   MT                 2   \n",
       "4         Я не ожидал, что Том предаст меня.  either   MT                 7   \n",
       "\n",
       "   tgt_length  src_length             hyp_tokens                   tgt_tokens  \\\n",
       "0           5           5     [worry, temporary]           [worry, temporary]   \n",
       "1           8           8           [tom, never]       [tom, never, supposed]   \n",
       "2           6           5      [hard, work, tom]      [trouble, working, tom]   \n",
       "3           4           2        [water, please]                [like, water]   \n",
       "4           8           7  [expect, tom, betray]  [think, tom, would, betray]   \n",
       "\n",
       "                                         src_tokens  \n",
       "0             [не, волнуйся, это, только, временно]  \n",
       "1  [тома, никогда, нет, там, где, он, должен, быть]  \n",
       "2                 [мне, сложно, работать, с, томом]  \n",
       "3                                [воду, пожалуйста]  \n",
       "4          [я, не, ожидал, что, том, предаст, меня]  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a60fc0",
   "metadata": {},
   "source": [
    "#### Search for Specific Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b42c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examples with the number seven: ['He was born at 7 a.m. on June 5, 1970.', 'The population of Hong Kong is more than seven million people.', 'In 1951 at the Palace of Soviet Pioneers, British international master Robert Wade held a session of simultaneous play with 30 local children under 14 years old. After seven hours of MI Wade game, he managed to make 10 draws, losing the remaining 20 matches.', 'My mother had seven sons and four daughters, and she had five sisters.', 'My mother had seven sons and four daughters, and she had five sisters.']\n"
     ]
    }
   ],
   "source": [
    "# Search for Specific Patterns\n",
    "def search_text_column(pattern, data, column='hyp'):\n",
    "    return [row[column] for idx, row in data.iterrows() if re.search(pattern, row[column])]\n",
    "\n",
    "# Example: Find rows in 'hyp' containing numbers\n",
    "hyp_with_numbers = search_text_column(r'\\d+', train_df, column='hyp')\n",
    "\n",
    "# Pattern matching using regular expressions\n",
    "def search_pattern(pattern, column, data=train_df):\n",
    "    matches = []\n",
    "    for _, row in data.iterrows():\n",
    "        match = re.search(pattern, row[column])\n",
    "        if match:\n",
    "            matches.append(row[column])\n",
    "    return matches\n",
    "\n",
    "# Example pattern search\n",
    "matches = search_pattern(r'\\b(seven|7)\\b', 'hyp')\n",
    "print(\"Examples with the number seven:\", matches[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b76119",
   "metadata": {},
   "source": [
    "#### Length Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27888274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length Analysis (outlier detection)\n",
    "train_df['hyp_length'] = train_df['hyp'].apply(lambda x: len(x.split()))\n",
    "train_df['tgt_length'] = train_df['tgt'].apply(lambda x: len(x.split()))\n",
    "train_df['src_length'] = train_df['src'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16560f05-dbed-4161-b639-22d3b2c864bc",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b10f34f5-d898-406d-8602-e878740332f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:45:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.007513523101806641,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f1d0f18f9b1468693caf720912aab52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 15:45:38 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-03 15:45:38 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-03 15:45:38 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-03 15:45:38 INFO: Using device: cpu\n",
      "2024-11-03 15:45:38 INFO: Loading: tokenize\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 15:45:38 INFO: Loading: mwt\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 15:45:38 INFO: Loading: lemma\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-03 15:45:38 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization using Stanza\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma')\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92146ed4-05da-438e-a68c-ca5e837e2f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['hyp_lemmas'] = train_df['hyp'].apply(lambda x: ' '.join(lemmatize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "541fa1c6-97e3-494d-956a-7f98cbad7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tgt_lemmas'] = train_df['tgt'].apply(lambda x: ' '.join(lemmatize_text(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc7ed140-b9ec-4cfc-95c8-4cb3b7a2bd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply lemmatization to the 'src' column only if the task is not 'ML'\n",
    "train_df['src_lemmas'] = train_df.apply(lambda row: ' '.join(lemmatize_text(row['src'])) if row['task'] != 'ML' else row['src'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6a629655-92f6-4f45-b4e8-368705a938e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>hyp_length</th>\n",
       "      <th>tgt_length</th>\n",
       "      <th>src_length</th>\n",
       "      <th>hyp_tokens</th>\n",
       "      <th>tgt_tokens</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>hyp_lemmas</th>\n",
       "      <th>tgt_lemmas</th>\n",
       "      <th>src_lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't worry, it's only temporary.</td>\n",
       "      <td>Don't worry. It's only temporary.</td>\n",
       "      <td>Не волнуйся. Это только временно.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[worry, temporary]</td>\n",
       "      <td>[не, волнуйся, это, только, временно]</td>\n",
       "      <td>do not worry , it 's only temporary .</td>\n",
       "      <td>do not worry . it 's only temporary .</td>\n",
       "      <td>Нi deлнуeсy . Это eeлькe deемеeнy .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tom is never where he should be.</td>\n",
       "      <td>Tom is never where he's supposed to be.</td>\n",
       "      <td>Тома никогда нет там, где он должен быть.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>[tom, never]</td>\n",
       "      <td>[tom, never, supposed]</td>\n",
       "      <td>[тома, никогда, нет, там, где, он, должен, быть]</td>\n",
       "      <td>Tom be never where he should be .</td>\n",
       "      <td>Tom be never where he 's suppose to be .</td>\n",
       "      <td>Тома eик нет там , где оi eeлжеe быть .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It's hard for me to work with Tom.</td>\n",
       "      <td>I have trouble working with Tom.</td>\n",
       "      <td>Мне сложно работать с Томом.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>[hard, work, tom]</td>\n",
       "      <td>[trouble, working, tom]</td>\n",
       "      <td>[мне, сложно, работать, с, томом]</td>\n",
       "      <td>it 's hard for I to work with Tom .</td>\n",
       "      <td>I have trouble work with Tom .</td>\n",
       "      <td>Мне eeожнe deботeтy с Томом .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Water, please.</td>\n",
       "      <td>I'd like some water.</td>\n",
       "      <td>Воду, пожалуйста.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>[water, please]</td>\n",
       "      <td>[like, water]</td>\n",
       "      <td>[воду, пожалуйста]</td>\n",
       "      <td>water , please .</td>\n",
       "      <td>I would like some water .</td>\n",
       "      <td>Воду , dожаeуйсeа .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I didn't expect Tom to betray me.</td>\n",
       "      <td>I didn't think that Tom would betray me.</td>\n",
       "      <td>Я не ожидал, что Том предаст меня.</td>\n",
       "      <td>either</td>\n",
       "      <td>MT</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>[expect, tom, betray]</td>\n",
       "      <td>[think, tom, would, betray]</td>\n",
       "      <td>[я, не, ожидал, что, том, предаст, меня]</td>\n",
       "      <td>I do not expect Tom to betray I .</td>\n",
       "      <td>I do not think that Tom would betray I .</td>\n",
       "      <td>Я нi eeидаe , что Том eре меня .</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  hyp  \\\n",
       "0   Don't worry, it's only temporary.   \n",
       "1    Tom is never where he should be.   \n",
       "2  It's hard for me to work with Tom.   \n",
       "3                      Water, please.   \n",
       "4   I didn't expect Tom to betray me.   \n",
       "\n",
       "                                        tgt  \\\n",
       "0         Don't worry. It's only temporary.   \n",
       "1   Tom is never where he's supposed to be.   \n",
       "2          I have trouble working with Tom.   \n",
       "3                      I'd like some water.   \n",
       "4  I didn't think that Tom would betray me.   \n",
       "\n",
       "                                         src     ref task model  hyp_length  \\\n",
       "0          Не волнуйся. Это только временно.  either   MT                 5   \n",
       "1  Тома никогда нет там, где он должен быть.  either   MT                 7   \n",
       "2               Мне сложно работать с Томом.  either   MT                 8   \n",
       "3                          Воду, пожалуйста.  either   MT                 2   \n",
       "4         Я не ожидал, что Том предаст меня.  either   MT                 7   \n",
       "\n",
       "   tgt_length  src_length             hyp_tokens                   tgt_tokens  \\\n",
       "0           5           5     [worry, temporary]           [worry, temporary]   \n",
       "1           8           8           [tom, never]       [tom, never, supposed]   \n",
       "2           6           5      [hard, work, tom]      [trouble, working, tom]   \n",
       "3           4           2        [water, please]                [like, water]   \n",
       "4           8           7  [expect, tom, betray]  [think, tom, would, betray]   \n",
       "\n",
       "                                         src_tokens  \\\n",
       "0             [не, волнуйся, это, только, временно]   \n",
       "1  [тома, никогда, нет, там, где, он, должен, быть]   \n",
       "2                 [мне, сложно, работать, с, томом]   \n",
       "3                                [воду, пожалуйста]   \n",
       "4          [я, не, ожидал, что, том, предаст, меня]   \n",
       "\n",
       "                              hyp_lemmas  \\\n",
       "0  do not worry , it 's only temporary .   \n",
       "1      Tom be never where he should be .   \n",
       "2    it 's hard for I to work with Tom .   \n",
       "3                       water , please .   \n",
       "4      I do not expect Tom to betray I .   \n",
       "\n",
       "                                 tgt_lemmas  \\\n",
       "0     do not worry . it 's only temporary .   \n",
       "1  Tom be never where he 's suppose to be .   \n",
       "2            I have trouble work with Tom .   \n",
       "3                 I would like some water .   \n",
       "4  I do not think that Tom would betray I .   \n",
       "\n",
       "                                src_lemmas  \n",
       "0      Нi deлнуeсy . Это eeлькe deемеeнy .  \n",
       "1  Тома eик нет там , где оi eeлжеe быть .  \n",
       "2            Мне eeожнe deботeтy с Томом .  \n",
       "3                      Воду , dожаeуйсeа .  \n",
       "4         Я нi eeидаe , что Том eре меня .  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb895652-dc07-401a-8953-e75891cdf251",
   "metadata": {},
   "source": [
    "### scr improvments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449af9fc-6038-4bb8-806c-7e98f166f059",
   "metadata": {},
   "source": [
    "## Convert processed data to CoNLL-U format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034fbf3e-01f6-42c5-b8c2-d09e6b30753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(train_df['hyp'].iloc[0])  \n",
    "CoNLL.write_doc2conll(doc, \"preprocessed_data.conllu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5210578c",
   "metadata": {},
   "source": [
    "## Cleaner approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d335fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 12:29:49 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03075432777404785,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 29,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3eab581659d4e80b28da6befc6dcc38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-03 12:29:50 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-03 12:29:50 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-03 12:29:50 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-03 12:29:50 INFO: Using device: cpu\n",
      "2024-11-03 12:29:50 INFO: Loading: tokenize\n",
      "2024-11-03 12:29:50 INFO: Loading: mwt\n",
      "2024-11-03 12:29:50 INFO: Loading: pos\n",
      "2024-11-03 12:29:51 INFO: Loading: lemma\n",
      "2024-11-03 12:29:51 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Empty hypothesis in row 287\n",
      "Warning: Empty target in row 287\n",
      "Warning: Empty target in row 454\n",
      "Warning: Empty hypothesis in row 645\n",
      "Warning: Empty hypothesis in row 690\n",
      "Warning: Empty target in row 690\n",
      "Warning: Empty hypothesis in row 953\n",
      "Warning: Empty hypothesis in row 1165\n",
      "Warning: Empty hypothesis in row 1411\n",
      "Warning: Empty target in row 1411\n",
      "Warning: Empty hypothesis in row 1551\n",
      "Warning: Empty target in row 1551\n",
      "Warning: Empty hypothesis in row 1571\n",
      "Warning: Empty target in row 1571\n",
      "Warning: Empty target in row 1671\n",
      "Warning: Empty hypothesis in row 2364\n",
      "Warning: Empty target in row 2364\n",
      "Warning: Empty hypothesis in row 2447\n",
      "Warning: Empty target in row 2447\n",
      "Warning: Empty hypothesis in row 2683\n",
      "Warning: Empty hypothesis in row 2830\n",
      "Warning: Empty target in row 2830\n",
      "Warning: Empty hypothesis in row 3070\n",
      "Warning: Empty hypothesis in row 3513\n",
      "Warning: Empty hypothesis in row 3661\n",
      "Warning: Empty hypothesis in row 3674\n",
      "Warning: Empty target in row 3674\n",
      "Warning: Empty hypothesis in row 3881\n",
      "Warning: Empty target in row 3881\n",
      "Warning: Empty hypothesis in row 3968\n",
      "Warning: Empty target in row 3968\n",
      "Warning: Empty hypothesis in row 4077\n",
      "Warning: Empty target in row 4077\n",
      "Warning: Empty hypothesis in row 4200\n",
      "Warning: Empty target in row 4381\n",
      "Warning: Empty target in row 4409\n",
      "Warning: Empty hypothesis in row 4805\n",
      "Warning: Empty hypothesis in row 4851\n",
      "Warning: Empty target in row 4851\n",
      "Warning: Empty hypothesis in row 5014\n",
      "Warning: Empty target in row 5148\n",
      "Warning: Empty hypothesis in row 5168\n",
      "Warning: Empty target in row 5417\n",
      "Warning: Empty hypothesis in row 5436\n",
      "Warning: Empty hypothesis in row 5559\n",
      "Warning: Empty target in row 5559\n",
      "Warning: Empty hypothesis in row 5673\n",
      "Warning: Empty target in row 5673\n",
      "Warning: Empty hypothesis in row 5735\n",
      "Warning: Empty target in row 5735\n",
      "Warning: Empty hypothesis in row 5934\n",
      "Warning: Empty hypothesis in row 5954\n",
      "Warning: Empty target in row 5954\n",
      "Warning: Empty hypothesis in row 5962\n",
      "Warning: Empty target in row 5962\n",
      "Warning: Empty hypothesis in row 5994\n",
      "Warning: Empty target in row 5994\n",
      "Warning: Empty hypothesis in row 6304\n",
      "Warning: Empty hypothesis in row 6420\n",
      "Warning: Empty target in row 6420\n",
      "Warning: Empty hypothesis in row 6578\n",
      "Warning: Empty hypothesis in row 7178\n",
      "Warning: Empty hypothesis in row 7255\n",
      "Warning: Empty hypothesis in row 7275\n",
      "Warning: Empty target in row 7275\n",
      "Warning: Empty hypothesis in row 7281\n",
      "Warning: Empty target in row 7642\n",
      "Warning: Empty hypothesis in row 8119\n",
      "Warning: Empty target in row 8119\n",
      "Warning: Empty hypothesis in row 8133\n",
      "Warning: Empty hypothesis in row 8392\n",
      "Warning: Empty hypothesis in row 8446\n",
      "Warning: Empty hypothesis in row 8594\n",
      "Warning: Empty hypothesis in row 8804\n",
      "Warning: Empty hypothesis in row 9155\n",
      "Warning: Empty target in row 9155\n",
      "Warning: Empty target in row 9178\n",
      "Warning: Empty hypothesis in row 9253\n",
      "Warning: Empty target in row 9253\n",
      "Warning: Empty hypothesis in row 9276\n",
      "Warning: Empty target in row 9276\n",
      "Warning: Empty hypothesis in row 9344\n",
      "Warning: Empty target in row 9344\n",
      "Warning: Empty hypothesis in row 9566\n",
      "Warning: Empty target in row 9566\n",
      "Warning: Empty hypothesis in row 9595\n",
      "Warning: Empty target in row 9808\n",
      "Warning: Empty target in row 10268\n",
      "Warning: Empty target in row 10346\n",
      "Warning: Empty target in row 10518\n",
      "Warning: Empty target in row 10538\n",
      "Warning: Empty target in row 10593\n",
      "Warning: Empty target in row 10690\n",
      "Warning: Empty target in row 10867\n",
      "Warning: Empty target in row 10948\n",
      "Warning: Empty target in row 11208\n",
      "Warning: Empty hypothesis in row 11660\n",
      "Warning: Empty target in row 11805\n",
      "Warning: Empty target in row 11845\n",
      "Warning: Empty target in row 11980\n",
      "Warning: Empty target in row 12597\n",
      "Warning: Empty target in row 12935\n",
      "Warning: Empty target in row 13429\n",
      "Warning: Empty target in row 13839\n",
      "Warning: Empty target in row 14040\n",
      "Warning: Empty target in row 14158\n",
      "Warning: Empty target in row 14423\n",
      "Warning: Empty target in row 14433\n",
      "Warning: Empty target in row 14820\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "with open('train.model-agnostic.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize Stanza pipeline for lemmatization\n",
    "nlp = stanza.Pipeline('en', processors='tokenize,lemma,pos')\n",
    "\n",
    "# Initialize stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation_pattern = re.compile(r'[^\\w\\s]')\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuation\n",
    "    text = punctuation_pattern.sub('', text)\n",
    "    # Tokenize and remove stopwords\n",
    "    tokens = [word for word in word_tokenize(text) if word.lower() not in stop_words]\n",
    "    # Lemmatize using Stanza\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmas = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
    "    return lemmas\n",
    "\n",
    "# Process the dataset\n",
    "processed_data = []\n",
    "for i, row in enumerate(data):\n",
    "    hyp_lemmas = preprocess_text(row['hyp'])\n",
    "    tgt_lemmas = preprocess_text(row['tgt'])\n",
    "    src_lemmas = preprocess_text(row['src'])\n",
    "\n",
    "    # Inspect any issues or unusual patterns\n",
    "    if len(hyp_lemmas) == 0:\n",
    "        print(f\"Warning: Empty hypothesis in row {i}\")\n",
    "    if len(tgt_lemmas) == 0:\n",
    "        print(f\"Warning: Empty target in row {i}\")\n",
    "    if len(src_lemmas) == 0:\n",
    "        print(f\"Warning: Empty source in row {i}\")\n",
    "\n",
    "    # Append processed text to a structured list for saving\n",
    "    processed_data.append({\n",
    "        \"hyp\": hyp_lemmas,\n",
    "        \"tgt\": tgt_lemmas,\n",
    "        \"src\": src_lemmas,\n",
    "        \"task\": row['task'],\n",
    "        \"model\": row['model']\n",
    "    })\n",
    "\n",
    "# Convert processed data to CoNLL-U format\n",
    "conll_data = []\n",
    "for entry in processed_data:\n",
    "    for i, lemma in enumerate(entry['hyp']):\n",
    "        conll_data.append({\n",
    "            'id': i + 1,\n",
    "            'form': lemma,\n",
    "            'lemma': lemma,\n",
    "            'upostag': '_',\n",
    "            'xpostag': '_',\n",
    "            'feats': '_',\n",
    "            'head': '_',\n",
    "            'deprel': '_',\n",
    "            'deps': '_',\n",
    "            'misc': '_'\n",
    "        })\n",
    "    conll_data.append({'id': '', 'form': '', 'lemma': '', 'upostag': '', 'xpostag': '', 'feats': '', 'head': '', 'deprel': '', 'deps': '', 'misc': ''})\n",
    "\n",
    "# Save in CoNLL-U format\n",
    "conll_df = pd.DataFrame(conll_data)\n",
    "conll_df.to_csv('processed_data.conllu', sep='\\t', index=False, header=False)\n",
    "\n",
    "print(\"Processing complete. Data saved in 'processed_data.conllu'\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testenv)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
