{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9c1ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import stanza\n",
    "import re\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "\n",
    "# Load Datasets\n",
    "train_df = pd.read_json('dataset/train.model-agnostic.json')\n",
    "test_df = pd.read_json('dataset/test.model-agnostic.json')\n",
    "val_df = pd.read_json('dataset/val.model-agnostic.json')\n",
    "\n",
    "# Filter rows for specific tasks in each dataset\n",
    "train_df = train_df[train_df['task'].isin(['PG'])]\n",
    "# Remove unnecessary columns for preprocessing\n",
    "train_df = train_df.drop(columns=['tgt', 'model', 'ref', 'task'])\n",
    "\n",
    "test_df = test_df[test_df['task'].isin(['PG'])]\n",
    "# Remove unnecessary columns for preprocessing\n",
    "test_df = test_df.drop(columns=['tgt', 'task', 'labels', 'label', 'p(Hallucination)', 'id'])\n",
    "\n",
    "val_df = val_df[val_df['task'].isin(['PG'])]\n",
    "# Remove unnecessary columns for preprocessing\n",
    "val_df = val_df.drop(columns=['tgt', 'model', 'ref', 'task', 'labels', 'label', 'p(Hallucination)'])\n",
    "\n",
    "# Reset indices for easier handling\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "val_df = val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "312b6a0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You're not alone, claire- -</td>\n",
       "      <td>You're not alone, Claire.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who told you to throw acid at Vargas, hmmm?</td>\n",
       "      <td>Who told you to throw acid at Vargas, hmm?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>♪ Where the pure angel merges with the antic s...</td>\n",
       "      <td>Where the pure angel merges with the antic Sphinx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where is it written what is it I'm meant to be?</td>\n",
       "      <td>Where is it written what is it I'm meant to be</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We'll find the skipper and then we'll go home.</td>\n",
       "      <td>We'll find the skipper and then we'll go home.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>Yeah, I'm listening.</td>\n",
       "      <td>Yeah, I'm listening.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Time?</td>\n",
       "      <td>The time?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>Plague?</td>\n",
       "      <td>A plague?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>Tango, Tango.</td>\n",
       "      <td>Tango.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>It's either him or me.</td>\n",
       "      <td>Him or me.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hyp  \\\n",
       "0                           You're not alone, claire- -   \n",
       "1           Who told you to throw acid at Vargas, hmmm?   \n",
       "2     ♪ Where the pure angel merges with the antic s...   \n",
       "3       Where is it written what is it I'm meant to be?   \n",
       "4        We'll find the skipper and then we'll go home.   \n",
       "...                                                 ...   \n",
       "9995                               Yeah, I'm listening.   \n",
       "9996                                              Time?   \n",
       "9997                                            Plague?   \n",
       "9998                                      Tango, Tango.   \n",
       "9999                             It's either him or me.   \n",
       "\n",
       "                                                    src  \n",
       "0                             You're not alone, Claire.  \n",
       "1            Who told you to throw acid at Vargas, hmm?  \n",
       "2     Where the pure angel merges with the antic Sphinx  \n",
       "3        Where is it written what is it I'm meant to be  \n",
       "4        We'll find the skipper and then we'll go home.  \n",
       "...                                                 ...  \n",
       "9995                               Yeah, I'm listening.  \n",
       "9996                                          The time?  \n",
       "9997                                          A plague?  \n",
       "9998                                             Tango.  \n",
       "9999                                         Him or me.  \n",
       "\n",
       "[10000 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d2832d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src</th>\n",
       "      <th>hyp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Here she comes.</td>\n",
       "      <td>Here she comes, here she comes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Everything will be allright.</td>\n",
       "      <td>Everythings fine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm not familiar with who that is.</td>\n",
       "      <td>I am unfamiliar with who that is.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's turning me into a crazy person.</td>\n",
       "      <td>It turns me into madness.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I'm joking.</td>\n",
       "      <td>I'm--I'm joking.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>What's the rush?</td>\n",
       "      <td>Wh-What's the rush?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>Fortunately, I got a plan.</td>\n",
       "      <td>But luckily, I've got a plan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>I'll get my things.</td>\n",
       "      <td>I'll get my trinkets.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>Can anyone back you up on that?</td>\n",
       "      <td>Can anyone corroborate that?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>Oh Michael, you're scaring me.</td>\n",
       "      <td>Oh michael, you scare me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      src                                hyp\n",
       "0                         Here she comes.    Here she comes, here she comes.\n",
       "1            Everything will be allright.                  Everythings fine.\n",
       "2      I'm not familiar with who that is.  I am unfamiliar with who that is.\n",
       "3    It's turning me into a crazy person.          It turns me into madness.\n",
       "4                             I'm joking.                   I'm--I'm joking.\n",
       "..                                    ...                                ...\n",
       "370                      What's the rush?                Wh-What's the rush?\n",
       "371            Fortunately, I got a plan.      But luckily, I've got a plan.\n",
       "372                   I'll get my things.              I'll get my trinkets.\n",
       "373       Can anyone back you up on that?       Can anyone corroborate that?\n",
       "374        Oh Michael, you're scaring me.           Oh michael, you scare me\n",
       "\n",
       "[375 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "125d2eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>src</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I have not been contacted.</td>\n",
       "      <td>I haven't been contacted by anybody.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I thought you'd be surprised at me too.</td>\n",
       "      <td>I thought so, too.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Is she gonna be okay?</td>\n",
       "      <td>Is she gonna be okay?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How long before you're making that happen?</td>\n",
       "      <td>How long before you make that happen?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You've got a customer.</td>\n",
       "      <td>You've got a client.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>We don't have the money to risk it, all right?</td>\n",
       "      <td>We can't afford to risk it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Uh, just for a couple of days.</td>\n",
       "      <td>Eh, just a few days.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>I'm not in any of this at all.</td>\n",
       "      <td>I'm not involved in this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>Just breathe deep, and I'll be right there.</td>\n",
       "      <td>Just breathe deep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>I never told you that before.</td>\n",
       "      <td>I never told you that.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>125 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                hyp  \\\n",
       "0                        I have not been contacted.   \n",
       "1           I thought you'd be surprised at me too.   \n",
       "2                             Is she gonna be okay?   \n",
       "3        How long before you're making that happen?   \n",
       "4                            You've got a customer.   \n",
       "..                                              ...   \n",
       "120  We don't have the money to risk it, all right?   \n",
       "121                  Uh, just for a couple of days.   \n",
       "122                  I'm not in any of this at all.   \n",
       "123     Just breathe deep, and I'll be right there.   \n",
       "124                   I never told you that before.   \n",
       "\n",
       "                                       src  \n",
       "0     I haven't been contacted by anybody.  \n",
       "1                       I thought so, too.  \n",
       "2                    Is she gonna be okay?  \n",
       "3    How long before you make that happen?  \n",
       "4                     You've got a client.  \n",
       "..                                     ...  \n",
       "120            We can't afford to risk it.  \n",
       "121                   Eh, just a few days.  \n",
       "122              I'm not involved in this.  \n",
       "123                     Just breathe deep.  \n",
       "124                 I never told you that.  \n",
       "\n",
       "[125 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d911f428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.009518861770629883,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d211712e95c4fed84bf9994e184a146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 12:17:00 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-08 12:17:00 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-11-08 12:17:01 INFO: File exists: C:\\Users\\Admin\\stanza_resources\\en\\default.zip\n",
      "2024-11-08 12:17:03 INFO: Finished downloading models and saved to C:\\Users\\Admin\\stanza_resources\n",
      "2024-11-08 12:17:03 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0065119266510009766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0ec4d18066d4782a6a6b864595e4c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-08 12:17:04 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-08 12:17:04 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-08 12:17:04 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-08 12:17:04 INFO: Using device: cpu\n",
      "2024-11-08 12:17:04 INFO: Loading: tokenize\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-08 12:17:04 INFO: Loading: mwt\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-08 12:17:04 INFO: Loading: lemma\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-08 12:17:04 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load Stanza NLP models\n",
    "stanza.download('en')\n",
    "nlp_en = stanza.Pipeline('en', processors='tokenize,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "759ab151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Segmentation and Normalization Function\n",
    "def normalize_text(text):\n",
    "    expanded_text = contractions.fix(text).lower()  # Expand contractions and lowercase\n",
    "    return re.sub(r'[^\\w\\s]', '', expanded_text)  # Remove punctuation\n",
    "\n",
    "# Lemmatization Function\n",
    "def preprocess_text(text):\n",
    "    doc = nlp_en(text)\n",
    "    return [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
    "\n",
    "# Apply Preprocessing Steps to Dataset\n",
    "def preprocess_dataset(df):\n",
    "    df['hyp_normalized'] = df['hyp'].apply(normalize_text)\n",
    "    df['src_normalized'] = df['src'].apply(normalize_text)\n",
    "    \n",
    "    # Sentence Segmentation\n",
    "    df['hyp_sentences'] = df['hyp_normalized'].apply(sent_tokenize)\n",
    "    df['src_sentences'] = df['src_normalized'].apply(sent_tokenize)\n",
    "    \n",
    "    # Tokenization\n",
    "    df['hyp_tokens'] = df['hyp_sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "    df['src_tokens'] = df['src_sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "    \n",
    "    # Lemmatization\n",
    "    df['hyp_lemmas'] = df['hyp_normalized'].apply(preprocess_text)\n",
    "    df['src_lemmas'] = df['src_normalized'].apply(preprocess_text)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess each dataset\n",
    "train_df = preprocess_dataset(train_df)\n",
    "test_df = preprocess_dataset(test_df)\n",
    "val_df = preprocess_dataset(val_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234097e2",
   "metadata": {},
   "source": [
    "### Columns to Keep\n",
    "1. **hyp** and **src**: Original texts, important as the base for comparison.\n",
    "2. **hyp_normalized** and **src_normalized**: Lowercased and punctuation-removed text, useful for a quick comparison without worrying about case sensitivity or punctuation.\n",
    "3. **hyp_lemmas** and **src_lemmas**: Lemmatized versions of the text, useful for semantic comparisons and to see if any new concepts are introduced in `hyp` that weren’t in `src`.\n",
    "\n",
    "### Optional Columns\n",
    "1. **hyp_sentences** and **src_sentences**: If the data typically consists of multiple sentences per row, these could be helpful for sentence-level comparison. However, if the text is mostly single-sentence, they might be redundant.\n",
    "2. **hyp_tokens** and **src_tokens**: These columns are useful if you plan to do token-level analysis, such as checking for specific word overlaps. If your focus is mainly on lemmas (which are conceptually higher-level), you may not need tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a0523a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for Similarity Analysis\n",
    "def calculate_similarity(df):\n",
    "    # Join lemmas back into text format\n",
    "    hyp_lemmas_text = df['hyp_lemmas'].apply(lambda x: ' '.join(x))\n",
    "    src_lemmas_text = df['src_lemmas'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Cosine Similarity using TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    hyp_tfidf = vectorizer.fit_transform(hyp_lemmas_text)\n",
    "    src_tfidf = vectorizer.transform(src_lemmas_text)\n",
    "    df['cosine_similarity'] = cosine_similarity(hyp_tfidf, src_tfidf).diagonal()\n",
    "    \n",
    "    # Semantic Similarity using Sentence Transformers\n",
    "    model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "    src_embeddings = model.encode(src_lemmas_text.tolist(), convert_to_tensor=True)\n",
    "    hyp_embeddings = model.encode(hyp_lemmas_text.tolist(), convert_to_tensor=True)\n",
    "    df['semantic_similarity'] = [sim.item() for sim in util.pytorch_cos_sim(src_embeddings, hyp_embeddings).diag()]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply similarity calculations to each dataset\n",
    "train_df = calculate_similarity(train_df)\n",
    "test_df = calculate_similarity(test_df)\n",
    "val_df = calculate_similarity(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8284a4cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data:\n",
      "                                                 hyp  \\\n",
      "0                        You're not alone, claire- -   \n",
      "1        Who told you to throw acid at Vargas, hmmm?   \n",
      "2  ♪ Where the pure angel merges with the antic s...   \n",
      "3    Where is it written what is it I'm meant to be?   \n",
      "4     We'll find the skipper and then we'll go home.   \n",
      "\n",
      "                                                 src  \\\n",
      "0                          You're not alone, Claire.   \n",
      "1         Who told you to throw acid at Vargas, hmm?   \n",
      "2  Where the pure angel merges with the antic Sphinx   \n",
      "3     Where is it written what is it I'm meant to be   \n",
      "4     We'll find the skipper and then we'll go home.   \n",
      "\n",
      "                                      hyp_normalized  \\\n",
      "0                          you are not alone claire    \n",
      "1          who told you to throw acid at vargas hmmm   \n",
      "2   where the pure angel merges with the antic sp...   \n",
      "3    where is it written what is it i am meant to be   \n",
      "4  we will find the skipper and then we will go home   \n",
      "\n",
      "                                      src_normalized  \\\n",
      "0                           you are not alone claire   \n",
      "1           who told you to throw acid at vargas hmm   \n",
      "2  where the pure angel merges with the antic sphinx   \n",
      "3    where is it written what is it i am meant to be   \n",
      "4  we will find the skipper and then we will go home   \n",
      "\n",
      "                                       hyp_sentences  \\\n",
      "0                         [you are not alone claire]   \n",
      "1        [who told you to throw acid at vargas hmmm]   \n",
      "2  [ where the pure angel merges with the antic s...   \n",
      "3  [where is it written what is it i am meant to be]   \n",
      "4  [we will find the skipper and then we will go ...   \n",
      "\n",
      "                                       src_sentences  \\\n",
      "0                         [you are not alone claire]   \n",
      "1         [who told you to throw acid at vargas hmm]   \n",
      "2  [where the pure angel merges with the antic sp...   \n",
      "3  [where is it written what is it i am meant to be]   \n",
      "4  [we will find the skipper and then we will go ...   \n",
      "\n",
      "                                          hyp_tokens  \\\n",
      "0                   [[you, are, not, alone, claire]]   \n",
      "1  [[who, told, you, to, throw, acid, at, vargas,...   \n",
      "2  [[where, the, pure, angel, merges, with, the, ...   \n",
      "3  [[where, is, it, written, what, is, it, i, am,...   \n",
      "4  [[we, will, find, the, skipper, and, then, we,...   \n",
      "\n",
      "                                          src_tokens  \\\n",
      "0                   [[you, are, not, alone, claire]]   \n",
      "1  [[who, told, you, to, throw, acid, at, vargas,...   \n",
      "2  [[where, the, pure, angel, merges, with, the, ...   \n",
      "3  [[where, is, it, written, what, is, it, i, am,...   \n",
      "4  [[we, will, find, the, skipper, and, then, we,...   \n",
      "\n",
      "                                          hyp_lemmas  \\\n",
      "0                      [you, be, not, alone, claire]   \n",
      "1  [who, tell, you, to, throw, acid, at, vargas, ...   \n",
      "2  [where, the, pure, angel, merge, with, the, an...   \n",
      "3  [where, be, it, write, what, be, it, I, be, me...   \n",
      "4  [we, will, find, the, skipper, and, then, we, ...   \n",
      "\n",
      "                                          src_lemmas  cosine_similarity  \\\n",
      "0                      [you, be, not, alone, claire]                1.0   \n",
      "1  [who, tell, you, to, throw, acid, at, vargas, ...                1.0   \n",
      "2  [where, the, pure, angel, merge, with, the, an...                1.0   \n",
      "3  [where, be, it, write, what, be, it, I, be, me...                1.0   \n",
      "4  [we, will, find, the, skipper, and, then, we, ...                1.0   \n",
      "\n",
      "   semantic_similarity  \n",
      "0                  1.0  \n",
      "1                  1.0  \n",
      "2                  1.0  \n",
      "3                  1.0  \n",
      "4                  1.0  \n",
      "\n",
      "Test Data:\n",
      "                                    src                                hyp  \\\n",
      "0                       Here she comes.    Here she comes, here she comes.   \n",
      "1          Everything will be allright.                  Everythings fine.   \n",
      "2    I'm not familiar with who that is.  I am unfamiliar with who that is.   \n",
      "3  It's turning me into a crazy person.          It turns me into madness.   \n",
      "4                           I'm joking.                   I'm--I'm joking.   \n",
      "\n",
      "                     hyp_normalized                        src_normalized  \\\n",
      "0     here she comes here she comes                        here she comes   \n",
      "1                  everythings fine           everything will be allright   \n",
      "2  i am unfamiliar with who that is    i am not familiar with who that is   \n",
      "3          it turns me into madness  it is turning me into a crazy person   \n",
      "4                   i ami am joking                           i am joking   \n",
      "\n",
      "                        hyp_sentences                           src_sentences  \\\n",
      "0     [here she comes here she comes]                        [here she comes]   \n",
      "1                  [everythings fine]           [everything will be allright]   \n",
      "2  [i am unfamiliar with who that is]    [i am not familiar with who that is]   \n",
      "3          [it turns me into madness]  [it is turning me into a crazy person]   \n",
      "4                   [i ami am joking]                           [i am joking]   \n",
      "\n",
      "                                   hyp_tokens  \\\n",
      "0      [[here, she, comes, here, she, comes]]   \n",
      "1                       [[everythings, fine]]   \n",
      "2  [[i, am, unfamiliar, with, who, that, is]]   \n",
      "3            [[it, turns, me, into, madness]]   \n",
      "4                      [[i, ami, am, joking]]   \n",
      "\n",
      "                                        src_tokens  \\\n",
      "0                             [[here, she, comes]]   \n",
      "1               [[everything, will, be, allright]]   \n",
      "2    [[i, am, not, familiar, with, who, that, is]]   \n",
      "3  [[it, is, turning, me, into, a, crazy, person]]   \n",
      "4                                [[i, am, joking]]   \n",
      "\n",
      "                                 hyp_lemmas  \\\n",
      "0        [here, she, come, here, she, come]   \n",
      "1                       [everythings, fine]   \n",
      "2  [I, be, unfamiliar, with, who, that, be]   \n",
      "3              [it, turn, I, into, madness]   \n",
      "4                        [I, ami, be, joke]   \n",
      "\n",
      "                                    src_lemmas  cosine_similarity  \\\n",
      "0                            [here, she, come]           1.000000   \n",
      "1             [everything, will, be, allright]           0.000000   \n",
      "2  [I, be, not, familiar, with, who, that, be]           0.623217   \n",
      "3    [it, be, turn, I, into, a, crazy, person]           0.684911   \n",
      "4                                [I, be, joke]           0.697763   \n",
      "\n",
      "   semantic_similarity  \n",
      "0             0.966426  \n",
      "1             0.840201  \n",
      "2             0.971583  \n",
      "3             0.838776  \n",
      "4             0.848128  \n",
      "\n",
      "Validation Data:\n",
      "                                          hyp  \\\n",
      "0                  I have not been contacted.   \n",
      "1     I thought you'd be surprised at me too.   \n",
      "2                       Is she gonna be okay?   \n",
      "3  How long before you're making that happen?   \n",
      "4                      You've got a customer.   \n",
      "\n",
      "                                     src  \\\n",
      "0   I haven't been contacted by anybody.   \n",
      "1                     I thought so, too.   \n",
      "2                  Is she gonna be okay?   \n",
      "3  How long before you make that happen?   \n",
      "4                   You've got a client.   \n",
      "\n",
      "                               hyp_normalized  \\\n",
      "0                   i have not been contacted   \n",
      "1  i thought you would be surprised at me too   \n",
      "2                     is she going to be okay   \n",
      "3  how long before you are making that happen   \n",
      "4                     you have got a customer   \n",
      "\n",
      "                         src_normalized  \\\n",
      "0  i have not been contacted by anybody   \n",
      "1                      i thought so too   \n",
      "2               is she going to be okay   \n",
      "3  how long before you make that happen   \n",
      "4                 you have got a client   \n",
      "\n",
      "                                  hyp_sentences  \\\n",
      "0                   [i have not been contacted]   \n",
      "1  [i thought you would be surprised at me too]   \n",
      "2                     [is she going to be okay]   \n",
      "3  [how long before you are making that happen]   \n",
      "4                     [you have got a customer]   \n",
      "\n",
      "                            src_sentences  \\\n",
      "0  [i have not been contacted by anybody]   \n",
      "1                      [i thought so too]   \n",
      "2               [is she going to be okay]   \n",
      "3  [how long before you make that happen]   \n",
      "4                 [you have got a client]   \n",
      "\n",
      "                                          hyp_tokens  \\\n",
      "0                  [[i, have, not, been, contacted]]   \n",
      "1  [[i, thought, you, would, be, surprised, at, m...   \n",
      "2                   [[is, she, going, to, be, okay]]   \n",
      "3  [[how, long, before, you, are, making, that, h...   \n",
      "4                    [[you, have, got, a, customer]]   \n",
      "\n",
      "                                       src_tokens  \\\n",
      "0  [[i, have, not, been, contacted, by, anybody]]   \n",
      "1                         [[i, thought, so, too]]   \n",
      "2                [[is, she, going, to, be, okay]]   \n",
      "3  [[how, long, before, you, make, that, happen]]   \n",
      "4                   [[you, have, got, a, client]]   \n",
      "\n",
      "                                         hyp_lemmas  \\\n",
      "0                       [I, have, not, be, contact]   \n",
      "1  [I, think, you, would, be, surprise, at, I, too]   \n",
      "2                       [be, she, go, to, be, okay]   \n",
      "3  [how, long, before, you, be, make, that, happen]   \n",
      "4                     [you, have, get, a, customer]   \n",
      "\n",
      "                                     src_lemmas  cosine_similarity  \\\n",
      "0      [I, have, not, be, contact, by, anybody]           0.810003   \n",
      "1                           [I, think, so, too]           0.459074   \n",
      "2                   [be, she, go, to, be, okay]           1.000000   \n",
      "3  [how, long, before, you, make, that, happen]           0.989737   \n",
      "4                   [you, have, get, a, client]           0.671947   \n",
      "\n",
      "   semantic_similarity  \n",
      "0             0.845145  \n",
      "1             0.503400  \n",
      "2             1.000000  \n",
      "3             0.995342  \n",
      "4             0.967243  \n",
      "\n",
      "Train Data (Cosine Similarity < 1):\n",
      "                                                  hyp  \\\n",
      "5   Seymour's Darling is the third... and little A...   \n",
      "9                                   ¿Mabel's a slave?   \n",
      "13                         Oh, he's an In-Valid, sir.   \n",
      "15                                          HARRY ⁇ .   \n",
      "17                           It's Frankfurt, Germany.   \n",
      "\n",
      "                                                  src  \\\n",
      "5   Seymour's Darling is third... and little Arnie...   \n",
      "9                                   Is Mabel a slave?   \n",
      "13                             He's an In-Valid, sir.   \n",
      "15                                     HARRY Imperio.   \n",
      "17                                Frankfurt, Germany.   \n",
      "\n",
      "                                       hyp_normalized  \\\n",
      "5   seymours darling is the third and little arnie...   \n",
      "9                                      mabels a slave   \n",
      "13                            oh he is an invalid sir   \n",
      "15                                            harry     \n",
      "17                            it is frankfurt germany   \n",
      "\n",
      "                                       src_normalized  \\\n",
      "5   seymours darling is third and little arnie mov...   \n",
      "9                                    is mabel a slave   \n",
      "13                               he is an invalid sir   \n",
      "15                                      harry imperio   \n",
      "17                                  frankfurt germany   \n",
      "\n",
      "                                        hyp_sentences  \\\n",
      "5   [seymours darling is the third and little arni...   \n",
      "9                                    [mabels a slave]   \n",
      "13                          [oh he is an invalid sir]   \n",
      "15                                            [harry]   \n",
      "17                          [it is frankfurt germany]   \n",
      "\n",
      "                                        src_sentences  \\\n",
      "5   [seymours darling is third and little arnie mo...   \n",
      "9                                  [is mabel a slave]   \n",
      "13                             [he is an invalid sir]   \n",
      "15                                    [harry imperio]   \n",
      "17                                [frankfurt germany]   \n",
      "\n",
      "                                           hyp_tokens  \\\n",
      "5   [[seymours, darling, is, the, third, and, litt...   \n",
      "9                                [[mabels, a, slave]]   \n",
      "13                   [[oh, he, is, an, invalid, sir]]   \n",
      "15                                          [[harry]]   \n",
      "17                     [[it, is, frankfurt, germany]]   \n",
      "\n",
      "                                           src_tokens  \\\n",
      "5   [[seymours, darling, is, third, and, little, a...   \n",
      "9                             [[is, mabel, a, slave]]   \n",
      "13                       [[he, is, an, invalid, sir]]   \n",
      "15                                 [[harry, imperio]]   \n",
      "17                             [[frankfurt, germany]]   \n",
      "\n",
      "                                           hyp_lemmas  \\\n",
      "5   [seymour, darling, be, the, third, and, little...   \n",
      "9                                   [mabel, a, slave]   \n",
      "13                      [oh, he, be, a, invalid, sir]   \n",
      "15                                            [harry]   \n",
      "17                       [it, be, frankfurt, germany]   \n",
      "\n",
      "                                           src_lemmas  cosine_similarity  \\\n",
      "5   [seymour, darling, be, third, and, little, arn...           0.798683   \n",
      "9                               [be, mabel, a, slave]           0.989234   \n",
      "13                          [he, be, a, invalid, sir]           0.886771   \n",
      "15                                   [harry, imperio]           1.000000   \n",
      "17                               [frankfurt, germany]           0.958895   \n",
      "\n",
      "    semantic_similarity  \n",
      "5              0.896277  \n",
      "9              0.897224  \n",
      "13             0.897194  \n",
      "15             0.591986  \n",
      "17             0.834110  \n",
      "\n",
      "Test Data (Cosine Similarity < 1):\n",
      "                                    src                              hyp  \\\n",
      "1          Everything will be allright.                Everythings fine.   \n",
      "3  It's turning me into a crazy person.        It turns me into madness.   \n",
      "4                           I'm joking.                 I'm--I'm joking.   \n",
      "5                    I'm not faking it.      I'm pretending it's a fake.   \n",
      "7      Why'd you got to go and do that?  Why did you have to go do that?   \n",
      "\n",
      "                   hyp_normalized                        src_normalized  \\\n",
      "1                everythings fine           everything will be allright   \n",
      "3        it turns me into madness  it is turning me into a crazy person   \n",
      "4                 i ami am joking                           i am joking   \n",
      "5    i am pretending it is a fake                    i am not faking it   \n",
      "7  why did you have to go do that     why did you got to go and do that   \n",
      "\n",
      "                      hyp_sentences                           src_sentences  \\\n",
      "1                [everythings fine]           [everything will be allright]   \n",
      "3        [it turns me into madness]  [it is turning me into a crazy person]   \n",
      "4                 [i ami am joking]                           [i am joking]   \n",
      "5    [i am pretending it is a fake]                    [i am not faking it]   \n",
      "7  [why did you have to go do that]     [why did you got to go and do that]   \n",
      "\n",
      "                                  hyp_tokens  \\\n",
      "1                      [[everythings, fine]]   \n",
      "3           [[it, turns, me, into, madness]]   \n",
      "4                     [[i, ami, am, joking]]   \n",
      "5     [[i, am, pretending, it, is, a, fake]]   \n",
      "7  [[why, did, you, have, to, go, do, that]]   \n",
      "\n",
      "                                        src_tokens  \\\n",
      "1               [[everything, will, be, allright]]   \n",
      "3  [[it, is, turning, me, into, a, crazy, person]]   \n",
      "4                                [[i, am, joking]]   \n",
      "5                       [[i, am, not, faking, it]]   \n",
      "7    [[why, did, you, got, to, go, and, do, that]]   \n",
      "\n",
      "                               hyp_lemmas  \\\n",
      "1                     [everythings, fine]   \n",
      "3            [it, turn, I, into, madness]   \n",
      "4                      [I, ami, be, joke]   \n",
      "5       [I, be, pretend, it, be, a, fake]   \n",
      "7  [why, do, you, have, to, go, do, that]   \n",
      "\n",
      "                                   src_lemmas  cosine_similarity  \\\n",
      "1            [everything, will, be, allright]           0.000000   \n",
      "3   [it, be, turn, I, into, a, crazy, person]           0.684911   \n",
      "4                               [I, be, joke]           0.697763   \n",
      "5                    [I, be, not, faking, it]           0.330761   \n",
      "7  [why, do, you, get, to, go, and, do, that]           0.835685   \n",
      "\n",
      "   semantic_similarity  \n",
      "1             0.840201  \n",
      "3             0.838776  \n",
      "4             0.848128  \n",
      "5             0.760670  \n",
      "7             0.878200  \n",
      "\n",
      "Validation Data (Cosine Similarity < 1):\n",
      "                                                 hyp  \\\n",
      "0                         I have not been contacted.   \n",
      "1            I thought you'd be surprised at me too.   \n",
      "5  Have you seen Norman in the last two days, or ...   \n",
      "6  Eddie, I'm begging you, I'm gonna need you to ...   \n",
      "7                  So, what, we're in the clear now?   \n",
      "\n",
      "                                    src  \\\n",
      "0  I haven't been contacted by anybody.   \n",
      "1                    I thought so, too.   \n",
      "5                 Have you seen Norman?   \n",
      "6       Eddie, please, give me the gun.   \n",
      "7                      Are we resolved?   \n",
      "\n",
      "                                      hyp_normalized  \\\n",
      "0                          i have not been contacted   \n",
      "1         i thought you would be surprised at me too   \n",
      "5  have you seen norman in the last two days or what   \n",
      "6  eddie i am begging you i am going to need you ...   \n",
      "7                    so what we are in the clear now   \n",
      "\n",
      "                         src_normalized  \\\n",
      "0  i have not been contacted by anybody   \n",
      "1                      i thought so too   \n",
      "5                  have you seen norman   \n",
      "6          eddie please give me the gun   \n",
      "7                       are we resolved   \n",
      "\n",
      "                                       hyp_sentences  \\\n",
      "0                        [i have not been contacted]   \n",
      "1       [i thought you would be surprised at me too]   \n",
      "5  [have you seen norman in the last two days or ...   \n",
      "6  [eddie i am begging you i am going to need you...   \n",
      "7                  [so what we are in the clear now]   \n",
      "\n",
      "                            src_sentences  \\\n",
      "0  [i have not been contacted by anybody]   \n",
      "1                      [i thought so too]   \n",
      "5                  [have you seen norman]   \n",
      "6          [eddie please give me the gun]   \n",
      "7                       [are we resolved]   \n",
      "\n",
      "                                          hyp_tokens  \\\n",
      "0                  [[i, have, not, been, contacted]]   \n",
      "1  [[i, thought, you, would, be, surprised, at, m...   \n",
      "5  [[have, you, seen, norman, in, the, last, two,...   \n",
      "6  [[eddie, i, am, begging, you, i, am, going, to...   \n",
      "7         [[so, what, we, are, in, the, clear, now]]   \n",
      "\n",
      "                                       src_tokens  \\\n",
      "0  [[i, have, not, been, contacted, by, anybody]]   \n",
      "1                         [[i, thought, so, too]]   \n",
      "5                     [[have, you, seen, norman]]   \n",
      "6           [[eddie, please, give, me, the, gun]]   \n",
      "7                           [[are, we, resolved]]   \n",
      "\n",
      "                                          hyp_lemmas  \\\n",
      "0                        [I, have, not, be, contact]   \n",
      "1   [I, think, you, would, be, surprise, at, I, too]   \n",
      "5  [have, you, see, norman, in, the, last, two, d...   \n",
      "6  [eddie, I, be, beg, you, I, be, go, to, need, ...   \n",
      "7            [so, what, we, be, in, the, clear, now]   \n",
      "\n",
      "                                 src_lemmas  cosine_similarity  \\\n",
      "0  [I, have, not, be, contact, by, anybody]           0.810003   \n",
      "1                       [I, think, so, too]           0.459074   \n",
      "5                  [have, you, see, norman]           0.565588   \n",
      "6        [eddie, please, give, I, the, gun]           0.688351   \n",
      "7                         [be, we, resolve]           0.364677   \n",
      "\n",
      "   semantic_similarity  \n",
      "0             0.845145  \n",
      "1             0.503400  \n",
      "5             0.721772  \n",
      "6             0.828834  \n",
      "7             0.318276  \n"
     ]
    }
   ],
   "source": [
    "# Display results for each dataset\n",
    "print(\"Train Data:\")\n",
    "print(train_df.head())\n",
    "print(\"\\nTest Data:\")\n",
    "print(test_df.head())\n",
    "print(\"\\nValidation Data:\")\n",
    "print(val_df.head())\n",
    "\n",
    "# Filter rows where cosine_similarity is not 1.0 for further analysis\n",
    "filtered_train_non_one_similarity = train_df[train_df['semantic_similarity'] < 0.9]\n",
    "filtered_test_non_one_similarity = test_df[test_df['semantic_similarity'] < 0.9]\n",
    "filtered_val_non_one_similarity = val_df[val_df['semantic_similarity'] < 0.9]\n",
    "\n",
    "# Display filtered rows\n",
    "print(\"\\nTrain Data (Cosine Similarity < 1):\")\n",
    "print(filtered_train_non_one_similarity.head())\n",
    "print(\"\\nTest Data (Cosine Similarity < 1):\")\n",
    "print(filtered_test_non_one_similarity.head())\n",
    "print(\"\\nValidation Data (Cosine Similarity < 1):\")\n",
    "print(filtered_val_non_one_similarity.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5d630a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv('generated_files/train_df.csv')\n",
    "test_df.to_csv('generated_files/test_df.csv')\n",
    "val_df.to_csv('generated_files/val_df.csv')\n",
    "# filtered_train_non_one_similarity.to_csv('generated_files/filtered_train_non_one_similarity.csv')\n",
    "# filtered_test_non_one_similarity.to_csv('generated_files/filtered_test_non_one_similarity.csv')\n",
    "# filtered_val_non_one_similarity.to_csv('generated_files/filtered_val_non_one_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e631a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testenv)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
