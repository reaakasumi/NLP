{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c1ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Imports\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import stanza\n",
    "import re\n",
    "import contractions\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load Dataset\n",
    "train_df = pd.read_json('train.model-agnostic.json')\n",
    "\n",
    "# Select rows where the 'task' column is either 'MT' or 'PG'\n",
    "filtered_df = train_df[train_df['task'].isin(['PG'])]\n",
    "\n",
    "# Reset the index \n",
    "filtered_df = filtered_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d911f428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00805211067199707,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e656f86ae78942a2909b531b73acabe1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:56:02 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-07 16:56:02 INFO: Downloading default packages for language: en (English) ...\n",
      "2024-11-07 16:56:03 INFO: File exists: C:\\Users\\Admin\\stanza_resources\\en\\default.zip\n",
      "2024-11-07 16:56:06 INFO: Finished downloading models and saved to C:\\Users\\Admin\\stanza_resources\n",
      "2024-11-07 16:56:06 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006000041961669922,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json",
       "rate": null,
       "total": 48453,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66c166226ac4c629f8e97f88c6b2d98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-07 16:56:06 INFO: Downloaded file to C:\\Users\\Admin\\stanza_resources\\resources.json\n",
      "2024-11-07 16:56:06 WARNING: Language en package default expects mwt, which has been added\n",
      "2024-11-07 16:56:06 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2024-11-07 16:56:06 INFO: Using device: cpu\n",
      "2024-11-07 16:56:06 INFO: Loading: tokenize\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\tokenization\\trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-07 16:56:07 INFO: Loading: mwt\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\mwt\\trainer.py:201: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-07 16:56:07 INFO: Loading: lemma\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\testenv\\lib\\site-packages\\stanza\\models\\lemma\\trainer.py:239: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-07 16:56:07 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load Stanza NLP models\n",
    "stanza.download('en')\n",
    "nlp_en = stanza.Pipeline('en', processors='tokenize,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "759ab151",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Text Segmentation and Normalization\n",
    "def normalize_text(text):\n",
    "    # Expand contractions (e.g., \"you're\" to \"you are\")\n",
    "    expanded_text = contractions.fix(text)\n",
    "    # Lowercase the expanded text\n",
    "    expanded_text = expanded_text.lower()  \n",
    "    # Remove punctuation\n",
    "    expanded_text = re.sub(r'[^\\w\\s]', '', expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "# Apply normalization\n",
    "filtered_df['hyp_normalized'] = filtered_df['hyp'].apply(normalize_text)\n",
    "filtered_df['src_normalized'] = filtered_df['src'].apply(normalize_text)\n",
    "\n",
    "# Sentence Segmentation\n",
    "filtered_df['hyp_sentences'] = filtered_df['hyp_normalized'].apply(sent_tokenize)\n",
    "filtered_df['src_sentences'] = filtered_df['src_normalized'].apply(sent_tokenize)\n",
    "\n",
    "# Tokenization\n",
    "filtered_df['hyp_tokens'] = filtered_df['hyp_sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "filtered_df['src_tokens'] = filtered_df['src_sentences'].apply(lambda sentences: [word_tokenize(sentence) for sentence in sentences])\n",
    "\n",
    "# Lemmatization\n",
    "def preprocess_text(text):\n",
    "    doc = nlp_en(text)  \n",
    "    lemmas = [word.lemma for sentence in doc.sentences for word in sentence.words]\n",
    "    return lemmas\n",
    "\n",
    "# Apply lemmatization \n",
    "filtered_df['hyp_lemmas'] = filtered_df['hyp_normalized'].apply(preprocess_text)\n",
    "filtered_df['src_lemmas'] = filtered_df['src_normalized'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234097e2",
   "metadata": {},
   "source": [
    "### Columns to Keep\n",
    "1. **hyp** and **src**: Original texts, important as the base for comparison.\n",
    "2. **hyp_normalized** and **src_normalized**: Lowercased and punctuation-removed text, useful for a quick comparison without worrying about case sensitivity or punctuation.\n",
    "3. **hyp_lemmas** and **src_lemmas**: Lemmatized versions of the text, useful for semantic comparisons and to see if any new concepts are introduced in `hyp` that weren’t in `src`.\n",
    "\n",
    "### Optional Columns\n",
    "1. **hyp_sentences** and **src_sentences**: If the data typically consists of multiple sentences per row, these could be helpful for sentence-level comparison. However, if the text is mostly single-sentence, they might be redundant.\n",
    "2. **hyp_tokens** and **src_tokens**: These columns are useful if you plan to do token-level analysis, such as checking for specific word overlaps. If your focus is mainly on lemmas (which are conceptually higher-level), you may not need tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0523a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>hyp_normalized</th>\n",
       "      <th>src_normalized</th>\n",
       "      <th>hyp_sentences</th>\n",
       "      <th>src_sentences</th>\n",
       "      <th>hyp_tokens</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>hyp_lemmas</th>\n",
       "      <th>src_lemmas</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>You're not alone, claire- -</td>\n",
       "      <td></td>\n",
       "      <td>You're not alone, Claire.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>you are not alone claire</td>\n",
       "      <td>you are not alone claire</td>\n",
       "      <td>[you are not alone claire]</td>\n",
       "      <td>[you are not alone claire]</td>\n",
       "      <td>[[you, are, not, alone, claire]]</td>\n",
       "      <td>[[you, are, not, alone, claire]]</td>\n",
       "      <td>[you, be, not, alone, claire]</td>\n",
       "      <td>[you, be, not, alone, claire]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Who told you to throw acid at Vargas, hmmm?</td>\n",
       "      <td></td>\n",
       "      <td>Who told you to throw acid at Vargas, hmm?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>who told you to throw acid at vargas hmmm</td>\n",
       "      <td>who told you to throw acid at vargas hmm</td>\n",
       "      <td>[who told you to throw acid at vargas hmmm]</td>\n",
       "      <td>[who told you to throw acid at vargas hmm]</td>\n",
       "      <td>[[who, told, you, to, throw, acid, at, vargas,...</td>\n",
       "      <td>[[who, told, you, to, throw, acid, at, vargas,...</td>\n",
       "      <td>[who, tell, you, to, throw, acid, at, vargas, ...</td>\n",
       "      <td>[who, tell, you, to, throw, acid, at, vargas, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>♪ Where the pure angel merges with the antic s...</td>\n",
       "      <td></td>\n",
       "      <td>Where the pure angel merges with the antic Sphinx</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>where the pure angel merges with the antic sp...</td>\n",
       "      <td>where the pure angel merges with the antic sphinx</td>\n",
       "      <td>[ where the pure angel merges with the antic s...</td>\n",
       "      <td>[where the pure angel merges with the antic sp...</td>\n",
       "      <td>[[where, the, pure, angel, merges, with, the, ...</td>\n",
       "      <td>[[where, the, pure, angel, merges, with, the, ...</td>\n",
       "      <td>[where, the, pure, angel, merge, with, the, an...</td>\n",
       "      <td>[where, the, pure, angel, merge, with, the, an...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Where is it written what is it I'm meant to be?</td>\n",
       "      <td></td>\n",
       "      <td>Where is it written what is it I'm meant to be</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>where is it written what is it i am meant to be</td>\n",
       "      <td>where is it written what is it i am meant to be</td>\n",
       "      <td>[where is it written what is it i am meant to be]</td>\n",
       "      <td>[where is it written what is it i am meant to be]</td>\n",
       "      <td>[[where, is, it, written, what, is, it, i, am,...</td>\n",
       "      <td>[[where, is, it, written, what, is, it, i, am,...</td>\n",
       "      <td>[where, be, it, write, what, be, it, I, be, me...</td>\n",
       "      <td>[where, be, it, write, what, be, it, I, be, me...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We'll find the skipper and then we'll go home.</td>\n",
       "      <td></td>\n",
       "      <td>We'll find the skipper and then we'll go home.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>we will find the skipper and then we will go home</td>\n",
       "      <td>we will find the skipper and then we will go home</td>\n",
       "      <td>[we will find the skipper and then we will go ...</td>\n",
       "      <td>[we will find the skipper and then we will go ...</td>\n",
       "      <td>[[we, will, find, the, skipper, and, then, we,...</td>\n",
       "      <td>[[we, will, find, the, skipper, and, then, we,...</td>\n",
       "      <td>[we, will, find, the, skipper, and, then, we, ...</td>\n",
       "      <td>[we, will, find, the, skipper, and, then, we, ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 hyp tgt  \\\n",
       "0                        You're not alone, claire- -       \n",
       "1        Who told you to throw acid at Vargas, hmmm?       \n",
       "2  ♪ Where the pure angel merges with the antic s...       \n",
       "3    Where is it written what is it I'm meant to be?       \n",
       "4     We'll find the skipper and then we'll go home.       \n",
       "\n",
       "                                                 src  ref task model  \\\n",
       "0                          You're not alone, Claire.  src   PG         \n",
       "1         Who told you to throw acid at Vargas, hmm?  src   PG         \n",
       "2  Where the pure angel merges with the antic Sphinx  src   PG         \n",
       "3     Where is it written what is it I'm meant to be  src   PG         \n",
       "4     We'll find the skipper and then we'll go home.  src   PG         \n",
       "\n",
       "                                      hyp_normalized  \\\n",
       "0                          you are not alone claire    \n",
       "1          who told you to throw acid at vargas hmmm   \n",
       "2   where the pure angel merges with the antic sp...   \n",
       "3    where is it written what is it i am meant to be   \n",
       "4  we will find the skipper and then we will go home   \n",
       "\n",
       "                                      src_normalized  \\\n",
       "0                           you are not alone claire   \n",
       "1           who told you to throw acid at vargas hmm   \n",
       "2  where the pure angel merges with the antic sphinx   \n",
       "3    where is it written what is it i am meant to be   \n",
       "4  we will find the skipper and then we will go home   \n",
       "\n",
       "                                       hyp_sentences  \\\n",
       "0                         [you are not alone claire]   \n",
       "1        [who told you to throw acid at vargas hmmm]   \n",
       "2  [ where the pure angel merges with the antic s...   \n",
       "3  [where is it written what is it i am meant to be]   \n",
       "4  [we will find the skipper and then we will go ...   \n",
       "\n",
       "                                       src_sentences  \\\n",
       "0                         [you are not alone claire]   \n",
       "1         [who told you to throw acid at vargas hmm]   \n",
       "2  [where the pure angel merges with the antic sp...   \n",
       "3  [where is it written what is it i am meant to be]   \n",
       "4  [we will find the skipper and then we will go ...   \n",
       "\n",
       "                                          hyp_tokens  \\\n",
       "0                   [[you, are, not, alone, claire]]   \n",
       "1  [[who, told, you, to, throw, acid, at, vargas,...   \n",
       "2  [[where, the, pure, angel, merges, with, the, ...   \n",
       "3  [[where, is, it, written, what, is, it, i, am,...   \n",
       "4  [[we, will, find, the, skipper, and, then, we,...   \n",
       "\n",
       "                                          src_tokens  \\\n",
       "0                   [[you, are, not, alone, claire]]   \n",
       "1  [[who, told, you, to, throw, acid, at, vargas,...   \n",
       "2  [[where, the, pure, angel, merges, with, the, ...   \n",
       "3  [[where, is, it, written, what, is, it, i, am,...   \n",
       "4  [[we, will, find, the, skipper, and, then, we,...   \n",
       "\n",
       "                                          hyp_lemmas  \\\n",
       "0                      [you, be, not, alone, claire]   \n",
       "1  [who, tell, you, to, throw, acid, at, vargas, ...   \n",
       "2  [where, the, pure, angel, merge, with, the, an...   \n",
       "3  [where, be, it, write, what, be, it, I, be, me...   \n",
       "4  [we, will, find, the, skipper, and, then, we, ...   \n",
       "\n",
       "                                          src_lemmas  cosine_similarity  \\\n",
       "0                      [you, be, not, alone, claire]                1.0   \n",
       "1  [who, tell, you, to, throw, acid, at, vargas, ...                1.0   \n",
       "2  [where, the, pure, angel, merge, with, the, an...                1.0   \n",
       "3  [where, be, it, write, what, be, it, I, be, me...                1.0   \n",
       "4  [we, will, find, the, skipper, and, then, we, ...                1.0   \n",
       "\n",
       "   semantic_similarity  \n",
       "0                  1.0  \n",
       "1                  1.0  \n",
       "2                  1.0  \n",
       "3                  1.0  \n",
       "4                  1.0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join lemmas back into text format for each row only within the similarity analysis functions\n",
    "hyp_lemmas_text = filtered_df['hyp_lemmas'].apply(lambda x: ' '.join(x))\n",
    "src_lemmas_text = filtered_df['src_lemmas'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Similarity Analysis\n",
    "# Cosine Similarity using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "hyp_tfidf = vectorizer.fit_transform(hyp_lemmas_text)\n",
    "src_tfidf = vectorizer.transform(src_lemmas_text)\n",
    "filtered_df['cosine_similarity'] = cosine_similarity(hyp_tfidf, src_tfidf).diagonal()\n",
    "\n",
    "# Semantic Similarity using Sentence Transformers\n",
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v2')\n",
    "src_embeddings = model.encode(src_lemmas_text.tolist(), convert_to_tensor=True)\n",
    "hyp_embeddings = model.encode(hyp_lemmas_text.tolist(), convert_to_tensor=True)\n",
    "filtered_df['semantic_similarity'] = [sim.item() for sim in util.pytorch_cos_sim(src_embeddings, hyp_embeddings).diag()]\n",
    "\n",
    "# Display results\n",
    "filtered_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8284a4cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hyp</th>\n",
       "      <th>tgt</th>\n",
       "      <th>src</th>\n",
       "      <th>ref</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>hyp_normalized</th>\n",
       "      <th>src_normalized</th>\n",
       "      <th>hyp_sentences</th>\n",
       "      <th>src_sentences</th>\n",
       "      <th>hyp_tokens</th>\n",
       "      <th>src_tokens</th>\n",
       "      <th>hyp_lemmas</th>\n",
       "      <th>src_lemmas</th>\n",
       "      <th>cosine_similarity</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Seymour's Darling is the third... and little A...</td>\n",
       "      <td></td>\n",
       "      <td>Seymour's Darling is third... and little Arnie...</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>seymours darling is the third and little arnie...</td>\n",
       "      <td>seymours darling is third and little arnie mov...</td>\n",
       "      <td>[seymours darling is the third and little arni...</td>\n",
       "      <td>[seymours darling is third and little arnie mo...</td>\n",
       "      <td>[[seymours, darling, is, the, third, and, litt...</td>\n",
       "      <td>[[seymours, darling, is, third, and, little, a...</td>\n",
       "      <td>[seymour, darling, be, the, third, and, little...</td>\n",
       "      <td>[seymour, darling, be, third, and, little, arn...</td>\n",
       "      <td>0.798683</td>\n",
       "      <td>0.896277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>- Scud, do you read me, please?</td>\n",
       "      <td></td>\n",
       "      <td>Scud, do you read me?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>scud do you read me please</td>\n",
       "      <td>scud do you read me</td>\n",
       "      <td>[ scud do you read me please]</td>\n",
       "      <td>[scud do you read me]</td>\n",
       "      <td>[[scud, do, you, read, me, please]]</td>\n",
       "      <td>[[scud, do, you, read, me]]</td>\n",
       "      <td>[scud, do, you, read, I, please]</td>\n",
       "      <td>[scud, do, you, read, I]</td>\n",
       "      <td>0.900309</td>\n",
       "      <td>0.902591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>¿Mabel's a slave?</td>\n",
       "      <td></td>\n",
       "      <td>Is Mabel a slave?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>mabels a slave</td>\n",
       "      <td>is mabel a slave</td>\n",
       "      <td>[mabels a slave]</td>\n",
       "      <td>[is mabel a slave]</td>\n",
       "      <td>[[mabels, a, slave]]</td>\n",
       "      <td>[[is, mabel, a, slave]]</td>\n",
       "      <td>[mabel, a, slave]</td>\n",
       "      <td>[be, mabel, a, slave]</td>\n",
       "      <td>0.989234</td>\n",
       "      <td>0.897224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Homicide investigators have told me that they ...</td>\n",
       "      <td></td>\n",
       "      <td>Homicide investigators have told me that they ...</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>homicide investigators have told me that they ...</td>\n",
       "      <td>homicide investigators have told me that they ...</td>\n",
       "      <td>[homicide investigators have told me that they...</td>\n",
       "      <td>[homicide investigators have told me that they...</td>\n",
       "      <td>[[homicide, investigators, have, told, me, tha...</td>\n",
       "      <td>[[homicide, investigators, have, told, me, tha...</td>\n",
       "      <td>[homicide, investigator, have, tell, I, that, ...</td>\n",
       "      <td>[homicide, investigator, have, tell, I, that, ...</td>\n",
       "      <td>0.991717</td>\n",
       "      <td>0.998738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Oh, he's an In-Valid, sir.</td>\n",
       "      <td></td>\n",
       "      <td>He's an In-Valid, sir.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>oh he is an invalid sir</td>\n",
       "      <td>he is an invalid sir</td>\n",
       "      <td>[oh he is an invalid sir]</td>\n",
       "      <td>[he is an invalid sir]</td>\n",
       "      <td>[[oh, he, is, an, invalid, sir]]</td>\n",
       "      <td>[[he, is, an, invalid, sir]]</td>\n",
       "      <td>[oh, he, be, a, invalid, sir]</td>\n",
       "      <td>[he, be, a, invalid, sir]</td>\n",
       "      <td>0.886771</td>\n",
       "      <td>0.897194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9990</th>\n",
       "      <td>⁇  in?</td>\n",
       "      <td></td>\n",
       "      <td>Are you coming in?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>in</td>\n",
       "      <td>are you coming in</td>\n",
       "      <td>[   in]</td>\n",
       "      <td>[are you coming in]</td>\n",
       "      <td>[[in]]</td>\n",
       "      <td>[[are, you, coming, in]]</td>\n",
       "      <td>[in]</td>\n",
       "      <td>[be, you, come, in]</td>\n",
       "      <td>0.601023</td>\n",
       "      <td>0.602633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9991</th>\n",
       "      <td>- What'd he be seeing?</td>\n",
       "      <td></td>\n",
       "      <td>What did he see?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>what did he be seeing</td>\n",
       "      <td>what did he see</td>\n",
       "      <td>[ what did he be seeing]</td>\n",
       "      <td>[what did he see]</td>\n",
       "      <td>[[what, did, he, be, seeing]]</td>\n",
       "      <td>[[what, did, he, see]]</td>\n",
       "      <td>[what, do, he, be, see]</td>\n",
       "      <td>[what, do, he, see]</td>\n",
       "      <td>0.972964</td>\n",
       "      <td>0.987963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9994</th>\n",
       "      <td>I-I don't know who that guy is.</td>\n",
       "      <td></td>\n",
       "      <td>I don't know who that is.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>ii do not know who that guy is</td>\n",
       "      <td>i do not know who that is</td>\n",
       "      <td>[ii do not know who that guy is]</td>\n",
       "      <td>[i do not know who that is]</td>\n",
       "      <td>[[ii, do, not, know, who, that, guy, is]]</td>\n",
       "      <td>[[i, do, not, know, who, that, is]]</td>\n",
       "      <td>[ii, do, not, know, who, that, guy, be]</td>\n",
       "      <td>[I, do, not, know, who, that, be]</td>\n",
       "      <td>0.735315</td>\n",
       "      <td>0.863496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>Time?</td>\n",
       "      <td></td>\n",
       "      <td>The time?</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>time</td>\n",
       "      <td>the time</td>\n",
       "      <td>[time]</td>\n",
       "      <td>[the time]</td>\n",
       "      <td>[[time]]</td>\n",
       "      <td>[[the, time]]</td>\n",
       "      <td>[time]</td>\n",
       "      <td>[the, time]</td>\n",
       "      <td>0.829122</td>\n",
       "      <td>0.962874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>It's either him or me.</td>\n",
       "      <td></td>\n",
       "      <td>Him or me.</td>\n",
       "      <td>src</td>\n",
       "      <td>PG</td>\n",
       "      <td></td>\n",
       "      <td>it is either him or me</td>\n",
       "      <td>him or me</td>\n",
       "      <td>[it is either him or me]</td>\n",
       "      <td>[him or me]</td>\n",
       "      <td>[[it, is, either, him, or, me]]</td>\n",
       "      <td>[[him, or, me]]</td>\n",
       "      <td>[it, be, either, he, or, I]</td>\n",
       "      <td>[he, or, I]</td>\n",
       "      <td>0.597151</td>\n",
       "      <td>0.880067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7269 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    hyp tgt  \\\n",
       "5     Seymour's Darling is the third... and little A...       \n",
       "6                       - Scud, do you read me, please?       \n",
       "9                                     ¿Mabel's a slave?       \n",
       "12    Homicide investigators have told me that they ...       \n",
       "13                           Oh, he's an In-Valid, sir.       \n",
       "...                                                 ...  ..   \n",
       "9990                                             ⁇  in?       \n",
       "9991                             - What'd he be seeing?       \n",
       "9994                    I-I don't know who that guy is.       \n",
       "9996                                              Time?       \n",
       "9999                             It's either him or me.       \n",
       "\n",
       "                                                    src  ref task model  \\\n",
       "5     Seymour's Darling is third... and little Arnie...  src   PG         \n",
       "6                                 Scud, do you read me?  src   PG         \n",
       "9                                     Is Mabel a slave?  src   PG         \n",
       "12    Homicide investigators have told me that they ...  src   PG         \n",
       "13                               He's an In-Valid, sir.  src   PG         \n",
       "...                                                 ...  ...  ...   ...   \n",
       "9990                                 Are you coming in?  src   PG         \n",
       "9991                                   What did he see?  src   PG         \n",
       "9994                          I don't know who that is.  src   PG         \n",
       "9996                                          The time?  src   PG         \n",
       "9999                                         Him or me.  src   PG         \n",
       "\n",
       "                                         hyp_normalized  \\\n",
       "5     seymours darling is the third and little arnie...   \n",
       "6                            scud do you read me please   \n",
       "9                                        mabels a slave   \n",
       "12    homicide investigators have told me that they ...   \n",
       "13                              oh he is an invalid sir   \n",
       "...                                                 ...   \n",
       "9990                                                 in   \n",
       "9991                              what did he be seeing   \n",
       "9994                     ii do not know who that guy is   \n",
       "9996                                               time   \n",
       "9999                             it is either him or me   \n",
       "\n",
       "                                         src_normalized  \\\n",
       "5     seymours darling is third and little arnie mov...   \n",
       "6                                   scud do you read me   \n",
       "9                                      is mabel a slave   \n",
       "12    homicide investigators have told me that they ...   \n",
       "13                                 he is an invalid sir   \n",
       "...                                                 ...   \n",
       "9990                                  are you coming in   \n",
       "9991                                    what did he see   \n",
       "9994                          i do not know who that is   \n",
       "9996                                           the time   \n",
       "9999                                          him or me   \n",
       "\n",
       "                                          hyp_sentences  \\\n",
       "5     [seymours darling is the third and little arni...   \n",
       "6                         [ scud do you read me please]   \n",
       "9                                      [mabels a slave]   \n",
       "12    [homicide investigators have told me that they...   \n",
       "13                            [oh he is an invalid sir]   \n",
       "...                                                 ...   \n",
       "9990                                            [   in]   \n",
       "9991                           [ what did he be seeing]   \n",
       "9994                   [ii do not know who that guy is]   \n",
       "9996                                             [time]   \n",
       "9999                           [it is either him or me]   \n",
       "\n",
       "                                          src_sentences  \\\n",
       "5     [seymours darling is third and little arnie mo...   \n",
       "6                                 [scud do you read me]   \n",
       "9                                    [is mabel a slave]   \n",
       "12    [homicide investigators have told me that they...   \n",
       "13                               [he is an invalid sir]   \n",
       "...                                                 ...   \n",
       "9990                                [are you coming in]   \n",
       "9991                                  [what did he see]   \n",
       "9994                        [i do not know who that is]   \n",
       "9996                                         [the time]   \n",
       "9999                                        [him or me]   \n",
       "\n",
       "                                             hyp_tokens  \\\n",
       "5     [[seymours, darling, is, the, third, and, litt...   \n",
       "6                   [[scud, do, you, read, me, please]]   \n",
       "9                                  [[mabels, a, slave]]   \n",
       "12    [[homicide, investigators, have, told, me, tha...   \n",
       "13                     [[oh, he, is, an, invalid, sir]]   \n",
       "...                                                 ...   \n",
       "9990                                             [[in]]   \n",
       "9991                      [[what, did, he, be, seeing]]   \n",
       "9994          [[ii, do, not, know, who, that, guy, is]]   \n",
       "9996                                           [[time]]   \n",
       "9999                    [[it, is, either, him, or, me]]   \n",
       "\n",
       "                                             src_tokens  \\\n",
       "5     [[seymours, darling, is, third, and, little, a...   \n",
       "6                           [[scud, do, you, read, me]]   \n",
       "9                               [[is, mabel, a, slave]]   \n",
       "12    [[homicide, investigators, have, told, me, tha...   \n",
       "13                         [[he, is, an, invalid, sir]]   \n",
       "...                                                 ...   \n",
       "9990                           [[are, you, coming, in]]   \n",
       "9991                             [[what, did, he, see]]   \n",
       "9994                [[i, do, not, know, who, that, is]]   \n",
       "9996                                      [[the, time]]   \n",
       "9999                                    [[him, or, me]]   \n",
       "\n",
       "                                             hyp_lemmas  \\\n",
       "5     [seymour, darling, be, the, third, and, little...   \n",
       "6                      [scud, do, you, read, I, please]   \n",
       "9                                     [mabel, a, slave]   \n",
       "12    [homicide, investigator, have, tell, I, that, ...   \n",
       "13                        [oh, he, be, a, invalid, sir]   \n",
       "...                                                 ...   \n",
       "9990                                               [in]   \n",
       "9991                            [what, do, he, be, see]   \n",
       "9994            [ii, do, not, know, who, that, guy, be]   \n",
       "9996                                             [time]   \n",
       "9999                        [it, be, either, he, or, I]   \n",
       "\n",
       "                                             src_lemmas  cosine_similarity  \\\n",
       "5     [seymour, darling, be, third, and, little, arn...           0.798683   \n",
       "6                              [scud, do, you, read, I]           0.900309   \n",
       "9                                 [be, mabel, a, slave]           0.989234   \n",
       "12    [homicide, investigator, have, tell, I, that, ...           0.991717   \n",
       "13                            [he, be, a, invalid, sir]           0.886771   \n",
       "...                                                 ...                ...   \n",
       "9990                                [be, you, come, in]           0.601023   \n",
       "9991                                [what, do, he, see]           0.972964   \n",
       "9994                  [I, do, not, know, who, that, be]           0.735315   \n",
       "9996                                        [the, time]           0.829122   \n",
       "9999                                        [he, or, I]           0.597151   \n",
       "\n",
       "      semantic_similarity  \n",
       "5                0.896277  \n",
       "6                0.902591  \n",
       "9                0.897224  \n",
       "12               0.998738  \n",
       "13               0.897194  \n",
       "...                   ...  \n",
       "9990             0.602633  \n",
       "9991             0.987963  \n",
       "9994             0.863496  \n",
       "9996             0.962874  \n",
       "9999             0.880067  \n",
       "\n",
       "[7269 rows x 16 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter rows where cosine_similarity is not 1.0\n",
    "filtered_non_one_similarity = filtered_df[filtered_df['cosine_similarity'] < 1]\n",
    "\n",
    "# Display the filtered rows\n",
    "filtered_non_one_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d630a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('filtered_df.csv')\n",
    "filtered_non_one_similarity.to_csv('filtered_non_one_similarity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e631a89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (testenv)",
   "language": "python",
   "name": "testenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
