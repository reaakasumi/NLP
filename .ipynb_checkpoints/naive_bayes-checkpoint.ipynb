{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c92fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23624\\3616232120.py:43: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['hyp_lemmas'] = train_data['hyp_lemmas'].apply(ast.literal_eval)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23624\\3616232120.py:44: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['tgt_src_lemmas'] = train_data['tgt_src_lemmas'].apply(ast.literal_eval)\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_23624\\3616232120.py:49: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data['combined_text'] = train_data['hyp_lemmas'].apply(lambda x: \" \".join(x)) + \" \" + train_data['tgt_src_lemmas'].apply(lambda x: \" \".join(x))\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\hf_env\\lib\\site-packages\\sklearn\\base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Admin\\anaconda3\\envs\\hf_env\\lib\\site-packages\\sklearn\\utils\\_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n",
      "Validation Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    Hallucination       0.74      0.63      0.68      2971\n",
      "Not Hallucination       0.68      0.79      0.73      3022\n",
      "\n",
      "         accuracy                           0.71      5993\n",
      "        macro avg       0.71      0.71      0.70      5993\n",
      "     weighted avg       0.71      0.71      0.70      5993\n",
      "\n",
      "Best Parameters: {'nb__alpha': 0.5, 'tfidf__max_features': 10000, 'tfidf__min_df': 2, 'tfidf__ngram_range': (1, 2), 'tfidf__stop_words': None}\n",
      "Test Classification Report:\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "    Hallucination       0.76      0.53      0.62       998\n",
      "Not Hallucination       0.41      0.66      0.51       502\n",
      "\n",
      "         accuracy                           0.57      1500\n",
      "        macro avg       0.58      0.59      0.57      1500\n",
      "     weighted avg       0.64      0.57      0.58      1500\n",
      "\n",
      "Test data with actual and predicted labels:\n",
      "                                          combined_text              label  \\\n",
      "0             here she come here she come here she come  Not Hallucination   \n",
      "1          everythings fine everything will be allright      Hallucination   \n",
      "2     I be unfamiliar with who that be I be not fami...  Not Hallucination   \n",
      "3     it turn I into madness it be turn I into a cra...      Hallucination   \n",
      "4                               I ami be joke I be joke  Not Hallucination   \n",
      "...                                                 ...                ...   \n",
      "1495  organic compound biochemistry a peptide hormon...      Hallucination   \n",
      "1496                     full of sneer express contempt      Hallucination   \n",
      "1497        perfect in every way archaic wholly perfect      Hallucination   \n",
      "1498  medicine of or pertain to vasoocclusion pathol...      Hallucination   \n",
      "1499  slang a mustang a mutt dog of mix breed of lit...      Hallucination   \n",
      "\n",
      "        predicted_label  \n",
      "0     Not Hallucination  \n",
      "1     Not Hallucination  \n",
      "2     Not Hallucination  \n",
      "3         Hallucination  \n",
      "4     Not Hallucination  \n",
      "...                 ...  \n",
      "1495      Hallucination  \n",
      "1496      Hallucination  \n",
      "1497      Hallucination  \n",
      "1498      Hallucination  \n",
      "1499      Hallucination  \n",
      "\n",
      "[1500 rows x 3 columns]\n",
      "Predictions saved to 'test_data_with_predictions.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import ast\n",
    "\n",
    "def tune_hyperparameters(X_train, y_train):\n",
    "    \"\"\"Perform hyperparameter tuning using GridSearchCV.\"\"\"\n",
    "    # Define the pipeline for preprocessing and model training\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),  # Vectorizer for text data\n",
    "        ('nb', MultinomialNB())       # Naive Bayes classifier\n",
    "    ])\n",
    "\n",
    "    # Define the parameter grid for hyperparameter tuning\n",
    "    param_grid = {\n",
    "        'tfidf__max_features': [2000, 3000, 5000, 10000],\n",
    "        'tfidf__ngram_range': [(1, 1), (1, 2)],\n",
    "        'tfidf__min_df': [1, 2, 3],\n",
    "        'tfidf__stop_words': [None, stopwords.words('english')],\n",
    "        'nb__alpha': [1.0, 0.5, 0.1, 0.01]\n",
    "    }\n",
    "\n",
    "    # Perform grid search with stratified cross-validation\n",
    "    stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=stratified_cv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Return the best parameters and the best model from grid search\n",
    "    return grid_search.best_params_, grid_search.best_estimator_\n",
    "\n",
    "def preprocess_data(train_df, test_df):\n",
    "    \"\"\"Preprocess training and testing datasets.\"\"\"\n",
    "    # Drop missing values\n",
    "    train_data = train_df.dropna()\n",
    "    test_data = test_df.dropna()\n",
    "\n",
    "    # Convert string representations of lists to actual lists\n",
    "    train_data['hyp_lemmas'] = train_data['hyp_lemmas'].apply(ast.literal_eval)\n",
    "    train_data['tgt_src_lemmas'] = train_data['tgt_src_lemmas'].apply(ast.literal_eval)\n",
    "    test_data['hyp_lemmas'] = test_data['hyp_lemmas'].apply(ast.literal_eval)\n",
    "    test_data['tgt_src_lemmas'] = test_data['tgt_src_lemmas'].apply(ast.literal_eval)\n",
    "\n",
    "    # Combine `hyp_lemmas` and `res_lemmas` into a single text feature\n",
    "    train_data['combined_text'] = train_data['hyp_lemmas'].apply(lambda x: \" \".join(x)) + \" \" + train_data['tgt_src_lemmas'].apply(lambda x: \" \".join(x))\n",
    "    test_data['combined_text'] = test_data['hyp_lemmas'].apply(lambda x: \" \".join(x)) + \" \" + test_data['tgt_src_lemmas'].apply(lambda x: \" \".join(x))\n",
    "\n",
    "    # Extract features (text) and labels\n",
    "    X_train_full = train_data['combined_text']\n",
    "    y_train_full = train_data['label']\n",
    "\n",
    "    X_test = test_data['combined_text']\n",
    "    y_test = test_data['label']\n",
    "\n",
    "    return X_train_full, y_train_full, X_test, y_test, test_data\n",
    "\n",
    "def train_and_evaluate(X_train_full, y_train_full, X_test, y_test, test_data):\n",
    "    \"\"\"Train and evaluate the model with hyperparameter tuning.\"\"\"\n",
    "    # Split training data into training and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.2, random_state=42, stratify=y_train_full\n",
    "    )\n",
    "\n",
    "    # Vectorize text data using TF-IDF\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2), max_features=5000, stop_words=None)\n",
    "    X_train_vec = vectorizer.fit_transform(X_train)\n",
    "    X_val_vec = vectorizer.transform(X_val)\n",
    "    X_test_vec = vectorizer.transform(X_test)\n",
    "\n",
    "    # Handle class imbalance using SMOTE\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vec, y_train)\n",
    "\n",
    "    # Perform hyperparameter tuning to find the best model\n",
    "    best_params, best_model = tune_hyperparameters(X_train, y_train)\n",
    "\n",
    "    # Evaluate the best model on the validation set\n",
    "    y_val_pred = best_model.predict(X_val)\n",
    "    print(\"Validation Classification Report:\")\n",
    "    print(classification_report(y_val, y_val_pred))\n",
    "\n",
    "    # Retrain the best model on the full training data (including validation)\n",
    "    best_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "    # Evaluate the retrained model on the test set\n",
    "    y_test_pred = best_model.predict(X_test)\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(\"Test Classification Report:\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    # Add predicted labels to the test data for manual inspection\n",
    "    test_data['predicted_label'] = y_test_pred\n",
    "    print(\"Test data with actual and predicted labels:\")\n",
    "    print(test_data[['combined_text', 'label', 'predicted_label']])\n",
    "\n",
    "    # Save the test data with predictions to a CSV file\n",
    "    test_data.to_csv(\"test_data_with_predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to 'test_data_with_predictions.csv'.\")\n",
    "\n",
    "# Main process\n",
    "if __name__ == \"__main__\":\n",
    "    train_df = pd.read_csv('data/labeled_data/preprocessed/train_preprocessed.csv')\n",
    "    test_df = pd.read_csv('data/labeled_data/preprocessed/test_preprocessed.csv')\n",
    "\n",
    "    # Preprocess data and extract features and labels\n",
    "    X_train_full, y_train_full, X_test, y_test, test_data = preprocess_data(train_df, test_df)\n",
    "\n",
    "    # Train the model and evaluate its performance\n",
    "    train_and_evaluate(X_train_full, y_train_full, X_test, y_test, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ad83972d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Type  Count in Label  Count in Predicted\n",
      "0      Hallucinations             998                 699\n",
      "1  Non-Hallucinations             502                 801\n"
     ]
    }
   ],
   "source": [
    "# Reload the dataset\n",
    "data = pd.read_csv(\"test_data_with_predictions.csv\")\n",
    "\n",
    "# Count hallucinations and non-hallucinations in the label and predicted columns\n",
    "label_counts = data['label'].value_counts()\n",
    "predicted_counts = data['predicted_label'].value_counts()\n",
    "\n",
    "# Prepare the results as a DataFrame for clarity\n",
    "results = pd.DataFrame({\n",
    "    \"Type\": [\"Hallucinations\", \"Non-Hallucinations\"],\n",
    "    \"Count in Label\": [label_counts.get(\"Hallucination\", 0), label_counts.get(\"Not Hallucination\", 0)],\n",
    "    \"Count in Predicted\": [predicted_counts.get(\"Hallucination\", 0), predicted_counts.get(\"Not Hallucination\", 0)]\n",
    "})\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9441e236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Task Type  Hallucinations in Label  Non-Hallucinations in Label  \\\n",
      "0        DM                      434                          129   \n",
      "1        MT                      352                          210   \n",
      "2        PG                      212                          163   \n",
      "\n",
      "   Hallucinations in Predicted  Non-Hallucinations in Predicted  \n",
      "0                          551                               12  \n",
      "1                           86                              476  \n",
      "2                           62                              313  \n"
     ]
    }
   ],
   "source": [
    "# Count hallucinations and non-hallucinations per task type\n",
    "task_grouped = data.groupby(\"task\")\n",
    "\n",
    "# Initialize results dictionary\n",
    "task_results = {\n",
    "    \"Task Type\": [],\n",
    "    \"Hallucinations in Label\": [],\n",
    "    \"Non-Hallucinations in Label\": [],\n",
    "    \"Hallucinations in Predicted\": [],\n",
    "    \"Non-Hallucinations in Predicted\": []\n",
    "}\n",
    "\n",
    "# Iterate through each task type\n",
    "for task, group in task_grouped:\n",
    "    task_results[\"Task Type\"].append(task)\n",
    "    task_results[\"Hallucinations in Label\"].append((group['label'] == \"Hallucination\").sum())\n",
    "    task_results[\"Non-Hallucinations in Label\"].append((group['label'] == \"Not Hallucination\").sum())\n",
    "    task_results[\"Hallucinations in Predicted\"].append((group['predicted_label'] == \"Hallucination\").sum())\n",
    "    task_results[\"Non-Hallucinations in Predicted\"].append((group['predicted_label'] == \"Not Hallucination\").sum())\n",
    "\n",
    "# Convert results to a DataFrame\n",
    "task_results_df = pd.DataFrame(task_results)\n",
    "\n",
    "print(task_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3bc7caa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Task Type</th>\n",
       "      <th>Correct Hallucinations</th>\n",
       "      <th>Incorrect Hallucinations</th>\n",
       "      <th>Label Hallucinations</th>\n",
       "      <th>Predicted Label Hallucinations</th>\n",
       "      <th>Correct Non-Hallucinations</th>\n",
       "      <th>Label Non-Hallucinations</th>\n",
       "      <th>Predicted Label Non-Hallucinations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DM</td>\n",
       "      <td>427</td>\n",
       "      <td>131</td>\n",
       "      <td>434</td>\n",
       "      <td>551</td>\n",
       "      <td>5</td>\n",
       "      <td>129</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MT</td>\n",
       "      <td>62</td>\n",
       "      <td>314</td>\n",
       "      <td>352</td>\n",
       "      <td>86</td>\n",
       "      <td>186</td>\n",
       "      <td>210</td>\n",
       "      <td>476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PG</td>\n",
       "      <td>39</td>\n",
       "      <td>196</td>\n",
       "      <td>212</td>\n",
       "      <td>62</td>\n",
       "      <td>140</td>\n",
       "      <td>163</td>\n",
       "      <td>313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Task Type  Correct Hallucinations  Incorrect Hallucinations  \\\n",
       "0        DM                     427                       131   \n",
       "1        MT                      62                       314   \n",
       "2        PG                      39                       196   \n",
       "\n",
       "   Label Hallucinations  Predicted Label Hallucinations  \\\n",
       "0                   434                             551   \n",
       "1                   352                              86   \n",
       "2                   212                              62   \n",
       "\n",
       "   Correct Non-Hallucinations  Label Non-Hallucinations  \\\n",
       "0                           5                       129   \n",
       "1                         186                       210   \n",
       "2                         140                       163   \n",
       "\n",
       "   Predicted Label Non-Hallucinations  \n",
       "0                                  12  \n",
       "1                                 476  \n",
       "2                                 313  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recalculate comparison metrics\n",
    "def recalculate_comparison(data, task_column=\"task\", label_column=\"label\", predicted_column=\"predicted_label\"):\n",
    "    results = {\n",
    "        \"Task Type\": [],\n",
    "        \"Correct Hallucinations\": [],\n",
    "        \"Incorrect Hallucinations\": [],\n",
    "        \"Label Hallucinations\": [],\n",
    "        \"Predicted Label Hallucinations\": [],\n",
    "        \"Correct Non-Hallucinations\": [],\n",
    "        \"Incorrect Non-Hallucinations\": [],\n",
    "        \"Label Non-Hallucinations\": [],\n",
    "        \"Predicted Label Non-Hallucinations\": [],\n",
    "    }\n",
    "    \n",
    "    grouped = data.groupby(task_column)\n",
    "    for task, group in grouped:\n",
    "        # Calculate correct and incorrect predictions\n",
    "        label_hallucinations = (group['label'] == \"Hallucination\").sum()\n",
    "        predicted_label_hallucinations = (group['predicted_label'] == \"Hallucination\").sum()\n",
    "        \n",
    "        label_non_hallucinations = (group['label'] == \"Not Hallucination\").sum()\n",
    "        predicted_label_non_hallucinations = (group['predicted_label'] == \"Not Hallucination\").sum()\n",
    "        \n",
    "        correct_hallucinations = ((group[label_column] == \"Hallucination\") & (group[predicted_column] == \"Hallucination\")).sum()\n",
    "        correct_non_hallucinations = ((group[label_column] == \"Not Hallucination\") & (group[predicted_column] == \"Not Hallucination\")).sum()\n",
    "        \n",
    "        incorrect_hallucinations = (label_hallucinations-correct_hallucinations)+(predicted_label_hallucinations-correct_hallucinations)\n",
    "        incorrect_non_hallucinations = (label_hallucinations-correct_hallucinations)+(predicted_label_hallucinations-correct_hallucinations)\n",
    "        \n",
    "        # Append results\n",
    "        results[\"Task Type\"].append(task)\n",
    "        \n",
    "        results[\"Correct Hallucinations\"].append(correct_hallucinations)\n",
    "        results[\"Incorrect Hallucinations\"].append(incorrect_hallucinations)\n",
    "        results[\"Label Hallucinations\"].append(label_hallucinations)\n",
    "        results[\"Predicted Label Hallucinations\"].append(predicted_label_hallucinations)\n",
    "        \n",
    "        results[\"Correct Non-Hallucinations\"].append(correct_non_hallucinations)\n",
    "        results[\"Incorrect Non-Hallucinations\"].append(incorrect_non_hallucinations)\n",
    "        results[\"Label Non-Hallucinations\"].append(label_non_hallucinations)\n",
    "        results[\"Predicted Label Non-Hallucinations\"].append(predicted_label_non_hallucinations)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "comparison_results = recalculate_comparison(data)\n",
    "\n",
    "comparison_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3688791",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (hf_env)",
   "language": "python",
   "name": "hf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
